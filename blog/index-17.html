<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Avil Page - Personal &amp; tech blog by Chillar Anand">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Avil Page (old posts, page 17) | Avil Page</title>
<link href="../assets/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="../assets/css/baguetteBox.min.css" rel="stylesheet" type="text/css">
<link href="../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../assets/css/theme.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../rss.xml">
<link rel="canonical" href="https://avilpage.com/blog/index-17.html">
<link rel="prev" href="." type="text/html">
<link rel="next" href="index-16.html" type="text/html">
<!--[if lt IE 9]><script src="../assets/js/html5.js"></script><![endif]--><style>

 h1 {
     font-weight: bold;
     font-size: 25px;
 }

</style>
</head>
<body>
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <!-- Menubar -->
    <nav class="navbar navbar-expand-md static-top mb-4
                navbar-dark
                bg-dark
                "><div class="container">
<!-- This keeps the margins nice -->
            <a class="navbar-brand" href="../">

                    <span id="blog-title">Avil Page</span>
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="bs-navbar">
                <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="../top-10.html" class="nav-link">Top 10</a>
                </li>
<li class="nav-item">
<a href="../archive.html" class="nav-link">Articles</a>
                </li>
<li class="nav-item">
<a href="../rss.xml" class="nav-link">RSS</a>

                    
                </li>
</ul>
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
        </div>
<!-- /.container -->
    </nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="row justify-content-md-center">
        <div class="body-content col col-lg-6">
            <!--Body content-->
            
    
    
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../2022/12/common-crawl-laptop-web-directory.html" class="u-url">Common Crawl on Laptop - Building Web Directory</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Chillar Anand
            </span></p>
            <p class="dateline">
            <a href="../2022/12/common-crawl-laptop-web-directory.html" rel="bookmark">
            <time class="published dt-published" datetime="2022-12-08T07:41:39+05:30" itemprop="datePublished" title="2022-12-08">2022-12-08</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>This series of posts discuss processing of common crawl dataset on laptop.</p>
<ol>
<li><a href="../2022/11/common-crawl-laptop-extract-subset.html">Extracting Subset of Common Crawl</a></li>
<li>
<a href="../2022/12/common-crawl-laptop-web-directory.html">Building web directory</a> (this post)</li>
</ol>
<h4>Introduction</h4>
<p>In the earlier post, we have extracted all telugu web page urls to a csv file. In this post, let's explore these urls and build a web directory from it.</p>
<h4>Explore Data</h4>
<p>Let's see how many urls are present in the extracted subset of data.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>wc<span class="w"> </span>-l<span class="w"> </span>telugu.csv
<span class="w">  </span><span class="m">852025</span><span class="w"> </span>telugu.csv<span class="w"> </span>
</pre></div>

<p>In the earlier post, we have installed <code>duckdb</code> and used it for processing parquet files. <code>duckdb</code> can execute SQL queries directly on csv file. Let's use it to explore the data stored in telugu.csv.</p>
<p>Let's see how many unique domains are present in the data.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>duckdb<span class="w"> </span>-c<span class="w"> </span><span class="s2">"""</span>
<span class="s2">    SELECT COUNT(DISTINCT url_host_name_reversed) as unique_sites</span>
<span class="s2">    FROM read_csv('telugu.csv', auto_detect = TRUE);</span>
<span class="s2">"""</span>
┌──────────────┐
│<span class="w"> </span>unique_sites<span class="w"> </span>│
├──────────────┤
│<span class="w"> </span><span class="m">13632</span><span class="w">        </span>│
└──────────────┘
</pre></div>

<p>There ~14k unique domains. Let's see page density across these domains.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>duckdb<span class="w"> </span>-c<span class="w"> </span><span class="s2">"""</span>
<span class="s2">SELECT count    AS page_count,</span>
<span class="s2">COUNT(*) AS sites</span>
<span class="s2">FROM (SELECT url_host_name_reversed, COUNT(*) AS count</span>
<span class="s2">FROM read_csv('te.csv', auto_detect = TRUE)</span>
<span class="s2">GROUP BY url_host_name_reversed) AS t</span>
<span class="s2">GROUP BY page_count</span>
<span class="s2">ORDER BY page_count;</span>
<span class="s2">"""</span>
┌────────────┬───────┐
│<span class="w"> </span>page_count<span class="w"> </span>│<span class="w"> </span>sites<span class="w"> </span>│
├────────────┼───────┤
│<span class="w"> </span><span class="m">1</span><span class="w">          </span>│<span class="w"> </span><span class="m">6326</span><span class="w">  </span>│
│<span class="w"> </span><span class="m">2</span><span class="w">          </span>│<span class="w"> </span><span class="m">1904</span><span class="w">  </span>│
│<span class="w"> </span><span class="m">3</span><span class="w">          </span>│<span class="w"> </span><span class="m">733</span><span class="w">   </span>│
│<span class="w"> </span><span class="m">4</span><span class="w">          </span>│<span class="w"> </span><span class="m">459</span><span class="w">   </span>│
│<span class="w"> </span><span class="m">5</span><span class="w">          </span>│<span class="w"> </span><span class="m">315</span><span class="w">   </span>│
</pre></div>

<p>About ~75% of the sites have less than 5 pages. It is highly unlikely that these sites complete content is in Telugu language. After manually checking a few of these sites, I found that there are a lot of false positives. </p>
<p>In the earlier post, we have extracted all pages where there is Telugu language content. Let's filter out pages where Telugu is <strong>primary</strong> language.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>duckdb<span class="w"> </span>-c<span class="w"> </span><span class="s2">"""</span>
<span class="s2">  COPY (</span>
<span class="s2">    SELECT * FROM read_csv('cct.csv', auto_detect=true) </span>
<span class="s2">    WHERE content_languages like 'tel%'</span>
<span class="s2">  ) TO 'te_primary.csv' (DELIMITER ',', HEADER TRUE);</span>
<span class="s2">"""</span>
</pre></div>

<div class="code"><pre class="code literal-block">$<span class="w"> </span>wc<span class="w"> </span>-l<span class="w"> </span>te_primary.csv
<span class="w">  </span><span class="m">573130</span><span class="w"> </span>te_primary.csv
</pre></div>

<div class="code"><pre class="code literal-block">$<span class="w"> </span>duckdb<span class="w"> </span>-c<span class="w"> </span><span class="s2">"SELECT COUNT(DISTINCT url_host_name_reversed) as unique_sites FROM read_csv('te_primary.csv', auto_detect = TRUE)"</span><span class="w">                           </span>
┌──────────────┐
│<span class="w"> </span>unique_sites<span class="w"> </span>│
├──────────────┤
│<span class="w"> </span><span class="m">5666</span><span class="w">         </span>│
└──────────────┘<span class="w">    </span>
</pre></div>

<p>Let's see how page density per domain has changed.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>duckdb<span class="w"> </span>-c<span class="w"> </span><span class="s2">"""</span>
<span class="s2">SELECT count    AS page_count,</span>
<span class="s2">COUNT(*) AS sites</span>
<span class="s2">FROM (SELECT url_host_name_reversed, COUNT(*) AS count</span>
<span class="s2">FROM read_csv('te_primary.csv', auto_detect = TRUE)</span>
<span class="s2">GROUP BY url_host_name_reversed) AS t</span>
<span class="s2">GROUP BY page_count</span>
<span class="s2">ORDER BY page_count</span>
<span class="s2">;</span>
<span class="s2">"""</span>
┌────────────┬───────┐
│<span class="w"> </span>page_count<span class="w"> </span>│<span class="w"> </span>sites<span class="w"> </span>│
├────────────┼───────┤
│<span class="w"> </span><span class="m">1</span><span class="w">          </span>│<span class="w"> </span><span class="m">2183</span><span class="w">  </span>│
│<span class="w"> </span><span class="m">2</span><span class="w">          </span>│<span class="w"> </span><span class="m">843</span><span class="w">   </span>│
│<span class="w"> </span><span class="m">3</span><span class="w">          </span>│<span class="w"> </span><span class="m">235</span><span class="w">   </span>│
│<span class="w"> </span><span class="m">4</span><span class="w">          </span>│<span class="w"> </span><span class="m">146</span><span class="w">   </span>│
│<span class="w"> </span><span class="m">5</span><span class="w">          </span>│<span class="w"> </span><span class="m">98</span><span class="w">    </span>│
</pre></div>

<p>Page density remains almost the same. </p>
<p>Let's filter out sites which have at least 5 pages in Telugu. This will eliminate a lot of false positives. Let's look at the most popular sites from the results.</p>
<div class="code"><pre class="code literal-block"><span class="w">   </span><span class="m">1</span><span class="w">   </span>│<span class="w"> </span>Rank,Domain,Open<span class="w"> </span>Page<span class="w"> </span>Rank
<span class="w">   </span><span class="m">2</span><span class="w">   </span>│<span class="w"> </span><span class="m">25</span>,support.google.com,8.55
<span class="w">   </span><span class="m">3</span><span class="w">   </span>│<span class="w"> </span><span class="m">57</span>,t.me,7.76
<span class="w">   </span><span class="m">4</span><span class="w">   </span>│<span class="w"> </span><span class="m">76</span>,chrome.google.com,7.49
<span class="w">   </span><span class="m">5</span><span class="w">   </span>│<span class="w"> </span><span class="m">163</span>,support.mozilla.org,6.99
<span class="w">   </span><span class="m">6</span><span class="w">   </span>│<span class="w"> </span><span class="m">170</span>,groups.google.com,6.94
</pre></div>

<p>A lot of unrelated domains are present here because there might be 10+ pages in telugu in these domains as well. But we don't need these.</p>
<p>Let's look at only home page(or translated home page) where primary content language is telugu.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>duckdb<span class="w"> </span>-c<span class="w"> </span><span class="s2">"""</span>
<span class="s2">  SELECT COUNT(distinct url) </span>
<span class="s2">  FROM read_csv('te_primary.csv', auto_detect=true) </span>
<span class="s2">  WHERE (url_path = '/' or url_path = '/te/') and url_query is null;</span>
<span class="s2">"""</span>
</pre></div>

<p>Now the domain count has reduced to 6k. Let's export these domains to csv file.</p>
<p>To categorize these domains, Common-crawl doesn't yet provide any kind of categorisation. For now, we can use Open PageRank to sort these domains based on rank. </p>
<p>We can download top 10 million domains from Open PageRank<sup id="fnref:pagerank"><a class="footnote-ref" href="../2022/12/common-crawl-laptop-web-directory.html#fn:pagerank">3</a></sup>. Here is a simple python script to extract telugu domains from the list.</p>
<div class="code"><pre class="code literal-block"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">domains_file</span> <span class="o">=</span> <span class="s1">'domains.csv'</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">domains_file</span><span class="p">,</span> <span class="s1">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">telugu_domains</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()]</span>

<span class="n">telugu_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'.'</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">domain</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'.'</span><span class="p">)))</span> <span class="k">for</span> <span class="n">domain</span> <span class="ow">in</span> <span class="n">telugu_domains</span><span class="p">]</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'t10m.csv'</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">'Domain'</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">telugu_domains</span><span class="p">)]</span>

<span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">'t10m_telugu.csv'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

<p>Now, we have list of all telugu domains sorted by rank. In the next post, we will use this list to categorize the domains.</p>
<div class="footnote">
<hr>
<ol>
<li id="fn:common-crawl">
<p><a href="https://commoncrawl.org">https://commoncrawl.org</a> <a class="footnote-backref" href="../2022/12/common-crawl-laptop-web-directory.html#fnref:common-crawl" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
<li id="fn:columnar-index-wiki">
<p><a href="https://en.wikipedia.org/wiki/Column-oriented_DBMS">https://en.wikipedia.org/wiki/Column-oriented_DBMS</a> <a class="footnote-backref" href="../2022/12/common-crawl-laptop-web-directory.html#fnref:columnar-index-wiki" title="Jump back to footnote 2 in the text">↩</a></p>
</li>
<li id="fn:pagerank">
<p><a href="https://www.domcop.com/openpagerank">https://www.domcop.com/openpagerank</a> <a class="footnote-backref" href="../2022/12/common-crawl-laptop-web-directory.html#fnref:pagerank" title="Jump back to footnote 3 in the text">↩</a></p>
</li>
<li id="fn:duckdb">
<p><a href="https://duckdb.org">https://duckdb.org</a> <a class="footnote-backref" href="../2022/12/common-crawl-laptop-web-directory.html#fnref:duckdb" title="Jump back to footnote 4 in the text">↩</a></p>
</li>
</ol>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../2022/11/common-crawl-laptop-extract-subset.html" class="u-url">Common Crawl On Laptop - Extracting Subset Of Data</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Chillar Anand
            </span></p>
            <p class="dateline">
            <a href="../2022/11/common-crawl-laptop-extract-subset.html" rel="bookmark">
            <time class="published dt-published" datetime="2022-11-17T06:41:39+05:30" itemprop="datePublished" title="2022-11-17">2022-11-17</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>This series of posts discuss processing of common crawl dataset on laptop.</p>
<ol>
<li>
<a href="../2022/11/common-crawl-laptop-extract-subset.html">Extracting Subset of Common Crawl</a> (this post)</li>
<li><a href="../2022/12/common-crawl-laptop-web-directory.html">Building web directory</a></li>
</ol>
<h4>Introduction</h4>
<p>Common Crawl(CC)<sup id="fnref:common-crawl"><a class="footnote-ref" href="../2022/11/common-crawl-laptop-extract-subset.html#fn:common-crawl">1</a></sup> is an open repository of web containing peta bytes of data since 2008. As the dataset is huge, most of the tutorials use AWS EMR/Athena to process the data.</p>
<p>In this post, let's learn how to extract a subset of data(entire telugu language web pages) and process it on our local machine.</p>
<h4>Exploring Common Crawl</h4>
<p>CC provides monthly data dumps in WARC format. Each crawl consists of about ~3 billion web pages with a compressed size of ~100 TB.</p>
<p>In addition to WARC files, CC provides index files as well as columnar index<sup id="fnref:columnar-index-wiki"><a class="footnote-ref" href="../2022/11/common-crawl-laptop-extract-subset.html#fn:columnar-index-wiki">2</a></sup> files so that users can easily search, filter and download the data.</p>
<h4>Common Crawl Index</h4>
<p>Each crawl index is spread over 300 files consisting of ~250 GB of data. For this post, let use the latest crawl which is <code>CC-MAIN-2022-40</code>.</p>
<p>The index files can be accessed from AWS S3 or https. We can use aws cli to list all the files along with the sizes.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>aws<span class="w"> </span>s3<span class="w"> </span>ls<span class="w"> </span>--recursive<span class="w"> </span>--human-readable<span class="w"> </span>--summarize<span class="w"> </span>s3://commoncrawl/cc-index/collections/CC-MAIN-2022-40
<span class="m">2022</span>-10-08<span class="w"> </span><span class="m">16</span>:07:59<span class="w">  </span><span class="m">621</span>.9<span class="w"> </span>MiB<span class="w"> </span>cc-index/collections/CC-MAIN-2022-40/indexes/cdx-00000.gz
<span class="m">2022</span>-10-08<span class="w"> </span><span class="m">16</span>:08:26<span class="w">  </span><span class="m">721</span>.6<span class="w"> </span>MiB<span class="w"> </span>cc-index/collections/CC-MAIN-2022-40/indexes/cdx-00001.gz
...
<span class="m">2022</span>-10-08<span class="w"> </span><span class="m">16</span>:42:39<span class="w">  </span><span class="m">146</span>.6<span class="w"> </span>MiB<span class="w"> </span>cc-index/collections/CC-MAIN-2022-40/indexes/cluster.idx
<span class="m">2022</span>-10-08<span class="w"> </span><span class="m">16</span>:42:33<span class="w">   </span><span class="m">30</span><span class="w"> </span>Bytes<span class="w"> </span>cc-index/collections/CC-MAIN-2022-40/metadata.yaml

Total<span class="w"> </span>Objects:<span class="w"> </span><span class="m">302</span>
<span class="w">   </span>Total<span class="w"> </span>Size:<span class="w"> </span><span class="m">236</span>.1<span class="w"> </span>GiB
</pre></div>

<p>Let's download an index file to our local machine and see how the data is arranged. We can use <code>aws</code> cli to download the data from s3 bucket or use wget to download it from https endpoint.</p>
<div class="code"><pre class="code literal-block"><span class="c1"># from s3</span>
$<span class="w"> </span>aws<span class="w"> </span>s3<span class="w"> </span>cp<span class="w"> </span>s3://commoncrawl/cc-index/collections/CC-MAIN-2022-40/indexes/cdx-00000.gz<span class="w"> </span>.

<span class="c1"># from https</span>
$<span class="w"> </span>wget<span class="w"> </span>https://data.commoncrawl.org/cc-index/collections/CC-MAIN-2022-40/indexes/cdx-00000.gz
</pre></div>

<p>Let's print top five lines of the file.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>zcat<span class="w"> </span>&lt;<span class="w"> </span>cdx-00000.gz<span class="w"> </span><span class="p">|</span><span class="w"> </span>head<span class="w"> </span>-n<span class="w"> </span><span class="m">5</span>
<span class="m">0</span>,1,184,137<span class="o">)</span>/1klikbet<span class="w"> </span><span class="m">20221005193707</span><span class="w"> </span><span class="o">{</span><span class="s2">"url"</span>:<span class="w"> </span><span class="s2">"http://137.184.1.0/1klikbet/"</span>,<span class="w"> </span><span class="s2">"mime"</span>:<span class="w"> </span><span class="s2">"text/html"</span>,<span class="w"> </span><span class="s2">"mime-detected"</span>:<span class="w"> </span><span class="s2">"text/html"</span>,<span class="w"> </span><span class="s2">"status"</span>:<span class="w"> </span><span class="s2">"200"</span>,<span class="w"> </span><span class="s2">"digest"</span>:<span class="w"> </span><span class="s2">"XTKGORHKLZCHDBBOMYCYYIZVRPMXNRII"</span>,<span class="w"> </span><span class="s2">"length"</span>:<span class="w"> </span><span class="s2">"7065"</span>,<span class="w"> </span><span class="s2">"offset"</span>:<span class="w"> </span><span class="s2">"83437"</span>,<span class="w"> </span><span class="s2">"filename"</span>:<span class="w"> </span><span class="s2">"crawl-data/CC-MAIN-2022-40/segments/1664030337663.75/warc/CC-MAIN-20221005172112-20221005202112-00011.warc.gz"</span>,<span class="w"> </span><span class="s2">"charset"</span>:<span class="w"> </span><span class="s2">"UTF-8"</span>,<span class="w"> </span><span class="s2">"languages"</span>:<span class="w"> </span><span class="s2">"ind"</span><span class="o">}</span>
<span class="m">0</span>,1,184,137<span class="o">)</span>/7meter<span class="w"> </span><span class="m">20221005192131</span><span class="w"> </span><span class="o">{</span><span class="s2">"url"</span>:<span class="w"> </span><span class="s2">"http://137.184.1.0/7meter/"</span>,<span class="w"> </span><span class="s2">"mime"</span>:<span class="w"> </span><span class="s2">"text/html"</span>,<span class="w"> </span><span class="s2">"mime-detected"</span>:<span class="w"> </span><span class="s2">"text/html"</span>,<span class="w"> </span><span class="s2">"status"</span>:<span class="w"> </span><span class="s2">"200"</span>,<span class="w"> </span><span class="s2">"digest"</span>:<span class="w"> </span><span class="s2">"KUJAMRT6MXYR3RTWRJTIWJ5T2ZUB3EBH"</span>,<span class="w"> </span><span class="s2">"length"</span>:<span class="w"> </span><span class="s2">"7456"</span>,<span class="w"> </span><span class="s2">"offset"</span>:<span class="w"> </span><span class="s2">"142680"</span>,<span class="w"> </span><span class="s2">"filename"</span>:<span class="w"> </span><span class="s2">"crawl-data/CC-MAIN-2022-40/segments/1664030337663.75/warc/CC-MAIN-20221005172112-20221005202112-00182.warc.gz"</span>,<span class="w"> </span><span class="s2">"charset"</span>:<span class="w"> </span><span class="s2">"UTF-8"</span>,<span class="w"> </span><span class="s2">"languages"</span>:<span class="w"> </span><span class="s2">"ind"</span><span class="o">}</span>
...
</pre></div>

<p>The last column of each line contains the language information. We can use these index files, and we can  extract all the lines containing <code>tel</code> language code.</p>
<h4>Columnar Index</h4>
<p>We can also use columnar index to filter out telugu language web pages. Let's download a single file from the index.</p>
<div class="code"><pre class="code literal-block"><span class="c1"># from s3</span>
$<span class="w"> </span>aws<span class="w"> </span>s3<span class="w"> </span>cp<span class="w"> </span>s3://commoncrawl/cc-index/table/cc-main/warc/crawl<span class="o">=</span>CC-MAIN-2022-40/subset<span class="o">=</span>warc/part-00000-26160df0-1827-4787-a515-95ecaa2c9688.c000.gz.parquet<span class="w"> </span>.

<span class="c1"># from https</span>
$<span class="w"> </span>wget<span class="w"> </span>https://data.commoncrawl.org/cc-index/table/cc-main/warc/crawl<span class="o">=</span>CC-MAIN-2022-40/subset<span class="o">=</span>warc/part-00000-26160df0-1827-4787-a515-95ecaa2c9688.c000.gz.parquet
</pre></div>

<p>We can use Python pandas to read the parquet file and filter out telugu language web pages. Columnar index has <code>content_languages</code> column which can be used to filter out telugu pages as shown below.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">"""</span>
<span class="s2">import pandas as pd</span>
<span class="s2">filename = 'part-00000-26160df0-1827-4787-a515-95ecaa2c9688.c000.gz.parquet'</span>
<span class="s2">df = pd.read_parquet(filename)</span>
<span class="s2">df = df[df['content_languages'].str.startswith('tel', na=False)]</span>
<span class="s2">df.to_csv('telugu.csv')</span>
<span class="s2">"""</span>
</pre></div>

<p>I have used Macbook M1 with local ISP(Internet Service Provider) to download and extract the index. It took around 7 minutes to download a single file and 2 minutes to extract the data. To process 300 index files, it takes ~2 days.</p>
<p>Let's see how we can speed it up.</p>
<h4>Improving Performance</h4>
<h5>Faster Downloads</h5>
<p>My Wi-Fi speed is ~4MBps when downloading the index file. To download faster, I have created t2.micro(free-tier) EC2 instance on AWS. In this machine, download speed is ~10MBps. We can use other instances, but I am trying to use only free resources. In this machine, single file download is taking ~3 minutes.</p>
<p>CC dataset is hosted in us-east-1 region. So, I have created a new t2.micro instance in us-east-1 region. This instance is taking &lt;20 seconds to download a single file. We can download entire index in less than 2 hours.</p>
<h5>Faster Performance</h5>
<p>To extract data from index files, we have used Python pandas without specifying the engine. By default, it uses <code>pyarrow</code> which is a bit slow. To improve speed we can use <code>fastparquet</code> as engine which is ~5x faster than <code>pyarrow</code>.</p>
<div class="code"><pre class="code literal-block"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">filename</span> <span class="o">=</span> <span class="s1">'part-00000-26160df0-1827-4787-a515-95ecaa2c9688.c000.gz.parquet'</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="s1">'fastparquet'</span><span class="p">)</span>
</pre></div>

<p>To get better performance, we can use duckdb. Duckdb is an in-process SQL OLAP DBMS and it can execute SQL queries directly on parquet files with <code>parquet</code> extension.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>brew<span class="w"> </span>install<span class="w"> </span>duckdb

$<span class="w"> </span>duckdb<span class="w"> </span>-c<span class="w"> </span><span class="s1">'INSTALL parquet;'</span>
</pre></div>

<p>We can write a simple SQL query to filter out the required rows.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>duckdb<span class="w"> </span>-c<span class="w"> </span><span class="s2">"""</span>
<span class="s2">LOAD parquet;</span>
<span class="s2">COPY (select * from PARQUET_SCAN('part-00000-26160df0-1827-4787-a515-95ecaa2c9688.c000.gz.parquet') where content_languages ilike '%tel%') TO 'telugu.csv' (DELIMITER ',', HEADER TRUE);</span>
<span class="s2">"""</span>
</pre></div>

<p>Duckdb can execute SQL queries on remote files as well with <code>httpfs</code> extension.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>duckdb<span class="w"> </span>-c<span class="w"> </span><span class="s1">'INSTALL httpfs;'</span>

$<span class="w"> </span>duckdb<span class="w"> </span>-c<span class="w"> </span><span class="s2">"""</span>
<span class="s2">    LOAD httpfs;</span>
<span class="s2">    LOAD parquet;</span>

<span class="s2">    COPY (select * from PARQUET_SCAN('s3://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2022-40/subset=warc/part-00001-26160df0-1827-4787-a515-95ecaa2c9688.c000.gz.parquet') where content_languages ilike '%tel%') TO 'telugu.csv' (DELIMITER ',', HEADER TRUE);"""</span>
<span class="s2">"""</span>
</pre></div>

<p>Duckdb can also read series of parquet files and treat them as a single table. We can use this feature to process all the index files in a single command.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>duckdb<span class="w"> </span>-c<span class="w"> </span><span class="s2">"""</span>
<span class="s2">    LOAD httpfs;</span>
<span class="s2">    LOAD parquet;</span>

<span class="s2">    SET s3_region='us-east-1';</span>
<span class="s2">    SET s3_access_key_id='s3_secret_access_key';</span>
<span class="s2">    SET s3_secret_access_key='s3_secret_access_key';</span>

<span class="s2">    COPY (select * from PARQUET_SCAN('s3://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2022-40/subset=warc/*.parquet') where content_languages ilike '%tel%') TO 'telugu.csv' (DELIMITER ',', HEADER TRUE);</span>
<span class="s2">"""</span>
</pre></div>

<p>Depending on the file size, duckdb takes 10-15 seconds to process a single file. Since we don't need all the columns for further data processing, we can limit columns to required 5 columns.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>duckdb<span class="w"> </span>-c<span class="w"> </span><span class="s2">"""</span>
<span class="s2">    COPY (select url, content_languages, warc_filename, warc_record_offset, warc_record_length from PARQUET_SCAN('s3://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2022-40/subset=warc/*.parquet') where content_languages ilike '%tel%') TO 'telugu.csv' (DELIMITER ',', HEADER TRUE);</span>
<span class="s2">"""</span>
</pre></div>

<p>By limiting columns<sup id="fnref:cc-gg"><a class="footnote-ref" href="../2022/11/common-crawl-laptop-extract-subset.html#fn:cc-gg">3</a></sup> there is another 65% improvement in performance. Now duckdb can process a file in 3 to 8 seconds depending on the size of the file. We can process entire index in ~20 minutes.</p>
<h4>Conclusion</h4>
<p>With a single command, we can extract a subset of index from CC in ~2 hours. So far we have processed all files in a single process. We can also parallelize the process using <code>parallel</code> to get faster results.</p>
<p>In the upcoming posts, let's see how we can fetch the data from WARC files using this index and do further data processing.</p>
<div class="footnote">
<hr>
<ol>
<li id="fn:common-crawl">
<p><a href="https://commoncrawl.org">https://commoncrawl.org</a> <a class="footnote-backref" href="../2022/11/common-crawl-laptop-extract-subset.html#fnref:common-crawl" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
<li id="fn:columnar-index-wiki">
<p><a href="https://en.wikipedia.org/wiki/Column-oriented_DBMS">https://en.wikipedia.org/wiki/Column-oriented_DBMS</a> <a class="footnote-backref" href="../2022/11/common-crawl-laptop-extract-subset.html#fnref:columnar-index-wiki" title="Jump back to footnote 2 in the text">↩</a></p>
</li>
<li id="fn:cc-gg">
<p><a href="https://groups.google.com/g/common-crawl/c/WYwkW97RM4s">https://groups.google.com/g/common-crawl/c/WYwkW97RM4s</a> <a class="footnote-backref" href="../2022/11/common-crawl-laptop-extract-subset.html#fnref:cc-gg" title="Jump back to footnote 3 in the text">↩</a></p>
</li>
</ol>
</div>
    </div>
    </article>
</div>
        <ul class="pager postindexpager clearfix">
<li class="previous"><a href="." rel="prev">Newer posts</a></li>
            <li class="next"><a href="index-16.html" rel="next">Older posts</a></li>
        </ul>
<!--End of body content--><footer id="footer"><div class="container align-items-center justify-content-center d-flex">

<footer class="footer"><a href="https://github.com/ChillarAnand">
<img src="../images/icons8-github.svg" alt="github-chillar-anand" height="34"></a>

  

<a href="https://stackoverflow.com/users/2698552/chillar-anand">
<img src="../images/icons8-so.svg" alt="github-chillar-anand" height="30"></a>

  

<a href="https://youtube.com/@avilpage">
<img src="../images/icons8-youtube.svg" alt="github-chillar-anand" height="34"></a>

  

<a href="https://linkedin.com/in/chillaranand">
<img src="../images/icons8-linkedin.svg" alt="github-chillar-anand" height="34"></a>

  

<a href="https://twitter.com/chillaranand">
<img src="../images/icons8-twitter.svg" alt="github-chillar-anand" height="34"></a>

</footer>
</div>

<br><br></footer>
</div>
    </div>
    </div>

                <script src="../assets/js/jquery.min.js"></script><script src="../assets/js/popper.min.js"></script><script src="../assets/js/bootstrap.min.js"></script><script src="../assets/js/baguetteBox.min.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script>
</body>
</html>
