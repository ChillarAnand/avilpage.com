<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Avil Page (Posts about command-line)</title><link>https://avilpage.com/</link><description></description><atom:link href="https://avilpage.com/tags/command-line.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Thu, 17 Nov 2022 12:33:21 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Common Crawl On Laptop - Extracting Subset Of Data</title><link>https://avilpage.com/2022/11/common-crawl-laptop-extract-subset.html</link><dc:creator>Chillar Anand</dc:creator><description>&lt;p&gt;This series of posts discuss processing of common crawl dataset on laptop.&lt;/p&gt;
&lt;h4&gt;Introduction&lt;/h4&gt;
&lt;p&gt;Common Crawl(CC)&lt;sup id="fnref:common-crawl"&gt;&lt;a class="footnote-ref" href="https://avilpage.com/2022/11/common-crawl-laptop-extract-subset.html#fn:common-crawl"&gt;1&lt;/a&gt;&lt;/sup&gt; is an open repository of web containing peta bytes of data since 2008. As the dataset is huge, most of the tutorials use AWS EMR/Athena to process the data.&lt;/p&gt;
&lt;p&gt;In this post, let's learn how to extract a subset of data(entire telugu language web pages) and process it on our local machine.&lt;/p&gt;
&lt;h4&gt;Exploring Common Crawl&lt;/h4&gt;
&lt;p&gt;CC provides monthly data dumps in WARC format. Each crawl consists of about ~3 billion web pages with a compressed size of ~100 TB.&lt;/p&gt;
&lt;p&gt;In addition to WARC files, CC provides index files as well as columnar index&lt;sup id="fnref:columnar-index-wiki"&gt;&lt;a class="footnote-ref" href="https://avilpage.com/2022/11/common-crawl-laptop-extract-subset.html#fn:columnar-index-wiki"&gt;2&lt;/a&gt;&lt;/sup&gt; files so that users can easily search, filter and download the data.&lt;/p&gt;
&lt;h4&gt;Common Crawl Index&lt;/h4&gt;
&lt;p&gt;Each crawl index is spread over 300 files consisting of ~250 GB of data. For this post, let use the latest crawl which is &lt;code&gt;CC-MAIN-2022-40&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The index files can be accessed from AWS S3 or https. We can use aws cli to list all the files along with the sizes.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;$ aws s3 ls --recursive --human-readable --summarize s3://commoncrawl/cc-index/collections/CC-MAIN-2022-40
&lt;span class="m"&gt;2022&lt;/span&gt;-10-08 &lt;span class="m"&gt;16&lt;/span&gt;:07:59  &lt;span class="m"&gt;621&lt;/span&gt;.9 MiB cc-index/collections/CC-MAIN-2022-40/indexes/cdx-00000.gz
&lt;span class="m"&gt;2022&lt;/span&gt;-10-08 &lt;span class="m"&gt;16&lt;/span&gt;:08:26  &lt;span class="m"&gt;721&lt;/span&gt;.6 MiB cc-index/collections/CC-MAIN-2022-40/indexes/cdx-00001.gz
...
&lt;span class="m"&gt;2022&lt;/span&gt;-10-08 &lt;span class="m"&gt;16&lt;/span&gt;:42:39  &lt;span class="m"&gt;146&lt;/span&gt;.6 MiB cc-index/collections/CC-MAIN-2022-40/indexes/cluster.idx
&lt;span class="m"&gt;2022&lt;/span&gt;-10-08 &lt;span class="m"&gt;16&lt;/span&gt;:42:33   &lt;span class="m"&gt;30&lt;/span&gt; Bytes cc-index/collections/CC-MAIN-2022-40/metadata.yaml

Total Objects: &lt;span class="m"&gt;302&lt;/span&gt;
   Total Size: &lt;span class="m"&gt;236&lt;/span&gt;.1 GiB
&lt;/pre&gt;
&lt;p&gt;Let's download an index file to our local machine and see how the data is arranged. We can use &lt;code&gt;aws&lt;/code&gt; cli to download the data from s3 bucket or use wget to download it from https endpoint.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="c1"&gt;# from s3&lt;/span&gt;
$ aws s3 cp s3://commoncrawl/cc-index/collections/CC-MAIN-2022-40/indexes/cdx-00000.gz .

&lt;span class="c1"&gt;# from https&lt;/span&gt;
$ wget https://data.commoncrawl.org/cc-index/collections/CC-MAIN-2022-40/indexes/cdx-00000.gz
&lt;/pre&gt;
&lt;p&gt;Let's print top five lines of the file.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;$ zcat &amp;lt; cdx-00000.gz &lt;span class="p"&gt;|&lt;/span&gt; head -n &lt;span class="m"&gt;5&lt;/span&gt;
&lt;span class="m"&gt;0&lt;/span&gt;,1,184,137&lt;span class="o"&gt;)&lt;/span&gt;/1klikbet &lt;span class="m"&gt;20221005193707&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"url"&lt;/span&gt;: &lt;span class="s2"&gt;"http://137.184.1.0/1klikbet/"&lt;/span&gt;, &lt;span class="s2"&gt;"mime"&lt;/span&gt;: &lt;span class="s2"&gt;"text/html"&lt;/span&gt;, &lt;span class="s2"&gt;"mime-detected"&lt;/span&gt;: &lt;span class="s2"&gt;"text/html"&lt;/span&gt;, &lt;span class="s2"&gt;"status"&lt;/span&gt;: &lt;span class="s2"&gt;"200"&lt;/span&gt;, &lt;span class="s2"&gt;"digest"&lt;/span&gt;: &lt;span class="s2"&gt;"XTKGORHKLZCHDBBOMYCYYIZVRPMXNRII"&lt;/span&gt;, &lt;span class="s2"&gt;"length"&lt;/span&gt;: &lt;span class="s2"&gt;"7065"&lt;/span&gt;, &lt;span class="s2"&gt;"offset"&lt;/span&gt;: &lt;span class="s2"&gt;"83437"&lt;/span&gt;, &lt;span class="s2"&gt;"filename"&lt;/span&gt;: &lt;span class="s2"&gt;"crawl-data/CC-MAIN-2022-40/segments/1664030337663.75/warc/CC-MAIN-20221005172112-20221005202112-00011.warc.gz"&lt;/span&gt;, &lt;span class="s2"&gt;"charset"&lt;/span&gt;: &lt;span class="s2"&gt;"UTF-8"&lt;/span&gt;, &lt;span class="s2"&gt;"languages"&lt;/span&gt;: &lt;span class="s2"&gt;"ind"&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="m"&gt;0&lt;/span&gt;,1,184,137&lt;span class="o"&gt;)&lt;/span&gt;/7meter &lt;span class="m"&gt;20221005192131&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"url"&lt;/span&gt;: &lt;span class="s2"&gt;"http://137.184.1.0/7meter/"&lt;/span&gt;, &lt;span class="s2"&gt;"mime"&lt;/span&gt;: &lt;span class="s2"&gt;"text/html"&lt;/span&gt;, &lt;span class="s2"&gt;"mime-detected"&lt;/span&gt;: &lt;span class="s2"&gt;"text/html"&lt;/span&gt;, &lt;span class="s2"&gt;"status"&lt;/span&gt;: &lt;span class="s2"&gt;"200"&lt;/span&gt;, &lt;span class="s2"&gt;"digest"&lt;/span&gt;: &lt;span class="s2"&gt;"KUJAMRT6MXYR3RTWRJTIWJ5T2ZUB3EBH"&lt;/span&gt;, &lt;span class="s2"&gt;"length"&lt;/span&gt;: &lt;span class="s2"&gt;"7456"&lt;/span&gt;, &lt;span class="s2"&gt;"offset"&lt;/span&gt;: &lt;span class="s2"&gt;"142680"&lt;/span&gt;, &lt;span class="s2"&gt;"filename"&lt;/span&gt;: &lt;span class="s2"&gt;"crawl-data/CC-MAIN-2022-40/segments/1664030337663.75/warc/CC-MAIN-20221005172112-20221005202112-00182.warc.gz"&lt;/span&gt;, &lt;span class="s2"&gt;"charset"&lt;/span&gt;: &lt;span class="s2"&gt;"UTF-8"&lt;/span&gt;, &lt;span class="s2"&gt;"languages"&lt;/span&gt;: &lt;span class="s2"&gt;"ind"&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;
...
&lt;/pre&gt;
&lt;p&gt;The last column of each line contains the language information. We can use these index files, and we can  extract all the lines containing &lt;code&gt;tel&lt;/code&gt; language code.&lt;/p&gt;
&lt;h4&gt;Columnar Index&lt;/h4&gt;
&lt;p&gt;We can also use columnar index to filter out telugu language web pages. Let's download a single file from the index.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="c1"&gt;# from s3&lt;/span&gt;
$ aws s3 cp s3://commoncrawl/cc-index/table/cc-main/warc/crawl&lt;span class="o"&gt;=&lt;/span&gt;CC-MAIN-2022-40/subset&lt;span class="o"&gt;=&lt;/span&gt;warc/part-00001-26160df0-1827-4787-a515-95ecaa2c9688.c000.gz.parquet .

&lt;span class="c1"&gt;# from https&lt;/span&gt;
$ wget https://data.commoncrawl.org/cc-index/table/cc-main/warc/crawl&lt;span class="o"&gt;=&lt;/span&gt;CC-MAIN-2022-40/subset&lt;span class="o"&gt;=&lt;/span&gt;warc/part-00001-26160df0-1827-4787-a515-95ecaa2c9688.c000.gz.parquet
&lt;/pre&gt;
&lt;p&gt;We can use Python pandas to read the parquet file and filter out telugu language web pages. Columnar index has &lt;code&gt;content_languages&lt;/code&gt; column which can be use to filter out telugu pages.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;$ python -c &lt;span class="s2"&gt;"""&lt;/span&gt;
&lt;span class="s2"&gt;import pandas as pd&lt;/span&gt;
&lt;span class="s2"&gt;filename = 'part-00000-26160df0-1827-4787-a515-95ecaa2c9688.c000.gz.parquet'&lt;/span&gt;
&lt;span class="s2"&gt;df = pd.read_parquet(filename)&lt;/span&gt;
&lt;span class="s2"&gt;df = df[df['content_languages'].str.startswith('tel', na=False)]&lt;/span&gt;
&lt;span class="s2"&gt;df.to_csv('telugu.csv')&lt;/span&gt;
&lt;span class="s2"&gt;"""&lt;/span&gt;
&lt;/pre&gt;
&lt;h4&gt;Improving Performance&lt;/h4&gt;
&lt;h5&gt;Faster Downloads&lt;/h5&gt;
&lt;p&gt;I have used Macbook M1 with local ISP to download and extract the index. It took around 7 minutes to download a single file and 2 minutes to extract the data. To process 300 index files, it takes ~2 days.&lt;/p&gt;
&lt;p&gt;Let's see how we can speed it up.&lt;/p&gt;
&lt;p&gt;My Wi-Fi speed is ~4MBps when downloading the index file. To download faster, I have created t2.micro(free-tier) EC2 instance on AWS. In this machine, download speed is ~10MBps. We can use other instances, but I am trying to use only free resources. In this machine, single file download is taking ~3 minutes.&lt;/p&gt;
&lt;p&gt;CC dataset is hosted in us-east-1 region. So, I have created a new t2.micro instance in us-east-1 region. This instance is taking &amp;lt;20 seconds to download a single file. We can download entire index in less than 2 hours.&lt;/p&gt;
&lt;h5&gt;Faster Performance&lt;/h5&gt;
&lt;p&gt;To extract data from index files, we have used Python pandas without specifying the engine. By default, it uses &lt;code&gt;pyarrow&lt;/code&gt; which is a bit slow. To improve speed we can use &lt;code&gt;fastparquet&lt;/code&gt; as engine which is ~5x faster than &lt;code&gt;pyarrow&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;filename&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'part-00000-26160df0-1827-4787-a515-95ecaa2c9688.c000.gz.parquet'&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_parquet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;engine&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'fastparquet'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;To get better performance, we can use duckdb. Duckdb can execute SQL queries directly on parquet files with &lt;code&gt;parquet&lt;/code&gt; extension.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;$ brew install duckdb

$ duckdb -c &lt;span class="s1"&gt;'INSTALL parquet;'&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;We can write a simple SQL query to filter out the required rows.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;$ duckdb -c &lt;span class="s2"&gt;"""&lt;/span&gt;
&lt;span class="s2"&gt;LOAD parquet;&lt;/span&gt;
&lt;span class="s2"&gt;COPY (select * from PARQUET_SCAN('part-00000-26160df0-1827-4787-a515-95ecaa2c9688.c000.gz.parquet') where content_languages ilike '%tel%') TO 'te0001.csv' (DELIMITER ',', HEADER TRUE);&lt;/span&gt;
&lt;span class="s2"&gt;"""&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Duckdb can execute SQL queries on remote files as well with &lt;code&gt;httpfs&lt;/code&gt; extension.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;$ duckdb -c &lt;span class="s1"&gt;'INSTALL httpfs;'&lt;/span&gt;

$ duckdb -c &lt;span class="s2"&gt;"""&lt;/span&gt;
&lt;span class="s2"&gt;    LOAD httpfs;&lt;/span&gt;
&lt;span class="s2"&gt;    LOAD parquet;&lt;/span&gt;

&lt;span class="s2"&gt;    COPY (select * from PARQUET_SCAN('s3://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2022-40/subset=warc/part-00001-26160df0-1827-4787-a515-95ecaa2c9688.c000.gz.parquet') where content_languages ilike '%tel%') TO 'telugu.csv' (DELIMITER ',', HEADER TRUE);"""&lt;/span&gt;
&lt;span class="s2"&gt;"""&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Duckdb can also read series of parquet files and treat them as a single table. We can use this feature to process all the index files in a single command.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;$ duckdb -c &lt;span class="s2"&gt;"""&lt;/span&gt;
&lt;span class="s2"&gt;    LOAD httpfs;&lt;/span&gt;
&lt;span class="s2"&gt;    LOAD parquet;&lt;/span&gt;

&lt;span class="s2"&gt;    SET s3_region='us-east-1';&lt;/span&gt;
&lt;span class="s2"&gt;    SET s3_access_key_id='s3_secret_access_key';&lt;/span&gt;
&lt;span class="s2"&gt;    SET s3_secret_access_key='s3_secret_access_key';&lt;/span&gt;

&lt;span class="s2"&gt;    COPY (select * from PARQUET_SCAN('s3://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2022-40/subset=warc/*.parquet') where content_languages ilike '%tel%') TO 'telugu.csv' (DELIMITER ',', HEADER TRUE);&lt;/span&gt;
&lt;span class="s2"&gt;"""&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Depending on the file size, duckdb takes 10-15 seconds to process a single file. With this single command, entire index can be processed in an hour. We can also parallelize the process for faster results.&lt;/p&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;With this single command, we can extract any subset of index from CC in &amp;lt; 3 hours. In the upcoming posts, let's see how we can fetch the data from WARC files using this index and do further data processing.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:common-crawl"&gt;
&lt;p&gt;&lt;a href="https://commoncrawl.org"&gt;https://commoncrawl.org&lt;/a&gt; &lt;a class="footnote-backref" href="https://avilpage.com/2022/11/common-crawl-laptop-extract-subset.html#fnref:common-crawl" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:columnar-index-wiki"&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Column-oriented_DBMS"&gt;https://en.wikipedia.org/wiki/Column-oriented_DBMS&lt;/a&gt; &lt;a class="footnote-backref" href="https://avilpage.com/2022/11/common-crawl-laptop-extract-subset.html#fnref:columnar-index-wiki" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>command-line</category><category>common-crawl</category><category>data-analysis</category><guid>https://avilpage.com/2022/11/common-crawl-laptop-extract-subset.html</guid><pubDate>Thu, 17 Nov 2022 01:11:39 GMT</pubDate></item><item><title>Verifying TLS Certificate Chain With OpenSSL</title><link>https://avilpage.com/2019/11/verify-tls-certificate-chain-with-openssl.html</link><dc:creator>Chillar Anand</dc:creator><description>&lt;h4&gt;Introduction&lt;/h4&gt;
&lt;p&gt;To communicate securely over the internet, HTTPS (HTTP over TLS) is used. A key component of HTTPS is Certificate authority (CA), which by issuing digital certificates acts as a trusted 3rd party between server(eg: google.com) and others(eg: mobiles, laptops).&lt;/p&gt;
&lt;p&gt;In this article, we will learn how to obtain certificates from a server and manually verify them on a laptop to establish a chain of trust.&lt;/p&gt;
&lt;h4&gt;Chain of Trust&lt;/h4&gt;
&lt;p&gt;TLS certificate chain typically consists of server certificate which is signed by intermediate certificate of CA which is inturn signed with CA root certificate.&lt;/p&gt;
&lt;p&gt;Using OpenSSL, we can gather the server and intermediate certificates sent by a server using the following command.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;$ openssl s_client -showcerts -connect avilpage.com:443

CONNECTED&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;00000006&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="nv"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; US, &lt;span class="nv"&gt;O&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; DigiCert Inc, &lt;span class="nv"&gt;OU&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; www.digicert.com, &lt;span class="nv"&gt;CN&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; DigiCert High Assurance EV Root CA
verify &lt;span class="k"&gt;return&lt;/span&gt;:1
&lt;span class="nv"&gt;depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="nv"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; US, &lt;span class="nv"&gt;O&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; DigiCert Inc, &lt;span class="nv"&gt;OU&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; www.digicert.com, &lt;span class="nv"&gt;CN&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; DigiCert SHA2 High Assurance Server CA
verify &lt;span class="k"&gt;return&lt;/span&gt;:1
&lt;span class="nv"&gt;depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="nv"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; US, &lt;span class="nv"&gt;ST&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; California, &lt;span class="nv"&gt;L&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; San Francisco, &lt;span class="nv"&gt;O&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"GitHub, Inc."&lt;/span&gt;, &lt;span class="nv"&gt;CN&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; www.github.com
verify &lt;span class="k"&gt;return&lt;/span&gt;:1
---
Certificate chain
 &lt;span class="m"&gt;0&lt;/span&gt; s:/C&lt;span class="o"&gt;=&lt;/span&gt;US/ST&lt;span class="o"&gt;=&lt;/span&gt;California/L&lt;span class="o"&gt;=&lt;/span&gt;San Francisco/O&lt;span class="o"&gt;=&lt;/span&gt;GitHub, Inc./CN&lt;span class="o"&gt;=&lt;/span&gt;www.github.com
   i:/C&lt;span class="o"&gt;=&lt;/span&gt;US/O&lt;span class="o"&gt;=&lt;/span&gt;DigiCert Inc/OU&lt;span class="o"&gt;=&lt;/span&gt;www.digicert.com/CN&lt;span class="o"&gt;=&lt;/span&gt;DigiCert SHA2 High Assurance Server CA
-----BEGIN CERTIFICATE-----
MIIHMTCCBhmgAwIBAgIQDf56dauo4GsS0tOc8
MQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlna
0wGjIChBWUMo0oHjqvbsezt3tkBigAVBRQHvF
aTrrJ67dru040my
-----END CERTIFICATE-----
 &lt;span class="m"&gt;1&lt;/span&gt; s:/C&lt;span class="o"&gt;=&lt;/span&gt;US/O&lt;span class="o"&gt;=&lt;/span&gt;DigiCert Inc/OU&lt;span class="o"&gt;=&lt;/span&gt;www.digicert.com/CN&lt;span class="o"&gt;=&lt;/span&gt;DigiCert SHA2 High Assurance Server CA
   i:/C&lt;span class="o"&gt;=&lt;/span&gt;US/O&lt;span class="o"&gt;=&lt;/span&gt;DigiCert Inc/OU&lt;span class="o"&gt;=&lt;/span&gt;www.digicert.com/CN&lt;span class="o"&gt;=&lt;/span&gt;DigiCert High Assurance EV Root CA
-----BEGIN CERTIFICATE-----
MIIEsTCCA5mgAwIBAgIQBOHnpNxc8vNtwCtC
MQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGln
0wGjIChBWUMo0oHjqvbsezt3tkBigAVBRQHv
&lt;span class="nv"&gt;cPUeybQ&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;
-----END CERTIFICATE-----

    Verify &lt;span class="k"&gt;return&lt;/span&gt; code: &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;ok&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;This command internally verfies if the certificate chain is valid. The output contains the server certificate and the intermediate certificate along with their issuer and subject. Copy both the certificates into &lt;code&gt;server.pem&lt;/code&gt; and &lt;code&gt;intermediate.pem&lt;/code&gt; files.&lt;/p&gt;
&lt;p&gt;We can decode these pem files and see the information in these certificates using&lt;/p&gt;
&lt;pre class="code literal-block"&gt;$ openssl x509 -noout -text -in server.crt

Certificate:
    Data:
        Version: &lt;span class="m"&gt;3&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;0x2&lt;span class="o"&gt;)&lt;/span&gt;
    Signature Algorithm: sha256WithRSAEncryption
    ----
&lt;/pre&gt;
&lt;p&gt;We can also get only the subject and issuer of the certificate with&lt;/p&gt;
&lt;pre class="code literal-block"&gt;$ openssl x509 -noout -subject -noout -issuer -in server.pem

&lt;span class="nv"&gt;subject&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;CN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;www.github.com
&lt;span class="nv"&gt;issuer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;CN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;DigiCert SHA2 High Assurance Server CA

$ openssl x509 -noout -subject -noout -issuer -in intermediate.pem

&lt;span class="nv"&gt;subject&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;CN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;DigiCert SHA2 High Assurance Server CA
&lt;span class="nv"&gt;issuer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;CN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;DigiCert High Assurance EV Root CA
&lt;/pre&gt;
&lt;p&gt;Now that we have both server and intermediate certificates at hand, we need to look for the relevant root certificate (in this case DigiCert High Assurance EV Root CA) in our system to verify these.&lt;/p&gt;
&lt;p&gt;If you are using a Linux machine, all the root certificate will readily available in &lt;code&gt;.pem&lt;/code&gt; format in &lt;code&gt;/etc/ssl/certs&lt;/code&gt; directory.&lt;/p&gt;
&lt;p&gt;If you are using a Mac, open &lt;code&gt;Keychain Access&lt;/code&gt;, search and export the relevant root certificate in &lt;code&gt;.pem&lt;/code&gt; format.&lt;/p&gt;
&lt;p algin="center"&gt;
&lt;img src="https://avilpage.com/images/tls-openssl1.png"&gt;
&lt;/p&gt;

&lt;p&gt;We have all the 3 certificates in the chain of trust and we can validate them with&lt;/p&gt;
&lt;pre class="code literal-block"&gt;$ openssl verify -verbose -CAfile root.pem -untrusted intermediate.pem server.pem
server.pem: OK
&lt;/pre&gt;
&lt;p&gt;If there is some issue with validation OpenSSL will throw an error with relevant information.&lt;/p&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;In this article, we learnt how to get certificates from the server and validate them with the root certificate using OpenSSL.&lt;/p&gt;</description><category>command-line</category><guid>https://avilpage.com/2019/11/verify-tls-certificate-chain-with-openssl.html</guid><pubDate>Sat, 30 Nov 2019 04:45:14 GMT</pubDate></item><item><title>Archive Million Pages With wget In Minutes</title><link>https://avilpage.com/2018/11/archive-millions-pages-wget-minutes.html</link><dc:creator>Chillar Anand</dc:creator><description>&lt;h4&gt;Introduction&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://github.com/webrecorder/webrecorder"&gt;webrecorder&lt;/a&gt;, &lt;a href="https://github.com/internetarchive/heritrix3"&gt;heritrix&lt;/a&gt;, &lt;a href="https://nutch.apache.org/"&gt;nutch&lt;/a&gt;, &lt;a href="https://scrapy.org/"&gt;scrapy&lt;/a&gt;, &lt;a href="https://github.com/gocolly/colly"&gt;colly&lt;/a&gt;, &lt;a href="https://github.com/scrapinghub/frontera"&gt;frontera&lt;/a&gt; are popular tools for large scale web crawling and archiving.&lt;/p&gt;
&lt;p&gt;These tools require some learning curve and some of them don't have inbuilt support for warc(&lt;a href="https://en.wikipedia.org/wiki/Web_ARChive"&gt;Web ARChive&lt;/a&gt;) output format.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;wget&lt;/code&gt; comes bundled with most *nix systems and has inbuilt support for warc output. In this article we will see how to quickly archive web pages with wget.&lt;/p&gt;
&lt;h4&gt;Archiving with wget&lt;/h4&gt;
&lt;p&gt;In previous article we have extracted a &lt;a href="https://avilpage.com/2018/11/comparision-alexa-majestic-domcorp-top-million-sites.html"&gt;superset of top 1 million domains&lt;/a&gt;. We can use that list or urls to archive. Save this list to a file called &lt;code&gt;urls.txt&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This can be archived with the following command.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="nv"&gt;file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;urls.txt
wget -i &lt;span class="nv"&gt;$file&lt;/span&gt; --warc-file&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$file&lt;/span&gt; -t &lt;span class="m"&gt;3&lt;/span&gt; --timeout&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt; -q -o /dev/null -O /dev/null
&lt;/pre&gt;
&lt;p&gt;wget has the ability to continue partially downloaded files. But this option won't work with warc output. So, it is better to split this list into small chunks and process them. One added advantage of this approach is we can parallely download multiple chunks with wget.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;mkdir -p chunks
split -l &lt;span class="m"&gt;1000&lt;/span&gt; urls.txt chunks/ -d --additional-suffix&lt;span class="o"&gt;=&lt;/span&gt;.txt -a &lt;span class="m"&gt;3&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;This will split the file into several chunks each containing 1000 urls. wget doesn't have multithreading support. We can write a for loop to schedule a seperate process for each chunk.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;file&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; `&lt;span class="nv"&gt;ls&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nv"&gt;r&lt;/span&gt; &lt;span class="nv"&gt;chunks&lt;/span&gt;&lt;span class="cm"&gt;/*.txt`&lt;/span&gt;
&lt;span class="cm"&gt;do&lt;/span&gt;
&lt;span class="cm"&gt;   wget -i $file --warc-file=$file -t 3 --timeout=4 -q -o /dev/null -O /dev/null &amp;amp;&lt;/span&gt;
&lt;span class="cm"&gt;done&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;To archive 1000 urls, it takes ~15 minutes. In less than 20 minutes, it will download entire million pages.&lt;/p&gt;
&lt;p&gt;Also, each process takes ~8MB of memory. To run 1000 process, a system needs 8GB+ memory. Otherwise, number of parallel processes should be reduced which increases overall run time.&lt;/p&gt;
&lt;p&gt;Each archive chunk will be ~150MB and consume lot of storage. All downloaded acrhives can be zipped to reduce storage.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;gzip *.warc
&lt;/pre&gt;
&lt;p&gt;Here is an idempotent shell script to download and archive files in batches.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="ch"&gt;#! /bin/sh&lt;/span&gt;

&lt;span class="nb"&gt;set&lt;/span&gt; -x

&lt;span class="nv"&gt;batch&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1000&lt;/span&gt;
&lt;span class="nv"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;expr &lt;span class="si"&gt;${#&lt;/span&gt;&lt;span class="nv"&gt;batch&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; - &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="nv"&gt;maxproc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;50&lt;/span&gt;
&lt;span class="nv"&gt;file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;urls.txt
&lt;span class="nv"&gt;dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;&lt;span class="s1"&gt;'/projects/chunks'&lt;/span&gt;&lt;span class="nv"&gt;$batch&lt;/span&gt;


mkdir -p &lt;span class="nv"&gt;$dir&lt;/span&gt;
split -l &lt;span class="nv"&gt;$batch&lt;/span&gt; &lt;span class="nv"&gt;$file&lt;/span&gt; &lt;span class="nv"&gt;$dir&lt;/span&gt;&lt;span class="s1"&gt;'/'&lt;/span&gt; -d --additional-suffix&lt;span class="o"&gt;=&lt;/span&gt;.txt -a &lt;span class="nv"&gt;$size&lt;/span&gt;
sleep &lt;span class="m"&gt;1&lt;/span&gt;

&lt;span class="nv"&gt;useragent&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'&lt;/span&gt;


&lt;span class="k"&gt;for&lt;/span&gt; file &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="sb"&gt;`&lt;/span&gt;ls -r &lt;span class="nv"&gt;$dir&lt;/span&gt;/*.txt&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="k"&gt;do&lt;/span&gt;
    &lt;span class="nv"&gt;warcfile&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$file&lt;/span&gt;&lt;span class="s1"&gt;'.warc'&lt;/span&gt;
    &lt;span class="nv"&gt;warczip&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$warcfile&lt;/span&gt;&lt;span class="s1"&gt;'.gz'&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; -f &lt;span class="nv"&gt;$warczip&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; -f &lt;span class="nv"&gt;$warcfile&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
        &lt;span class="k"&gt;continue&lt;/span&gt;
    &lt;span class="k"&gt;fi&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;pgrep wget -c&lt;span class="k"&gt;)&lt;/span&gt; -lt &lt;span class="nv"&gt;$maxproc&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
        &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="nv"&gt;$file&lt;/span&gt;
        wget -H &lt;span class="s2"&gt;"user-agent: &lt;/span&gt;&lt;span class="nv"&gt;$useragent&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; -i &lt;span class="nv"&gt;$file&lt;/span&gt; --warc-file&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$file&lt;/span&gt; -t &lt;span class="m"&gt;3&lt;/span&gt; --timeout&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt; -q -o /dev/null -O /dev/null &lt;span class="p"&gt;&amp;amp;&lt;/span&gt;
        sleep &lt;span class="m"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;
        sleep &lt;span class="m"&gt;300&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; filename &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="sb"&gt;`&lt;/span&gt;find &lt;span class="nv"&gt;$dir&lt;/span&gt; -name &lt;span class="s1"&gt;'*.warc'&lt;/span&gt; -mmin +5&lt;span class="sb"&gt;`&lt;/span&gt;
        &lt;span class="k"&gt;do&lt;/span&gt;
            gzip &lt;span class="nv"&gt;$filename&lt;/span&gt; -9
        &lt;span class="k"&gt;done&lt;/span&gt;
    &lt;span class="k"&gt;fi&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/pre&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;In this article, we have seen how to archive million pages with wget in few minutes.&lt;/p&gt;
&lt;p&gt;wget2 has multithreading support and &lt;a href="https://gitlab.com/gnuwget/wget2/issues/65"&gt;it might have warc output soon&lt;/a&gt;. With that, archiving with wget becomes much easier.&lt;/p&gt;</description><category>command-line</category><guid>https://avilpage.com/2018/11/archive-millions-pages-wget-minutes.html</guid><pubDate>Sun, 18 Nov 2018 11:51:21 GMT</pubDate></item><item><title>Alexa vs Domcop vs Majestic - Top Million Sites</title><link>https://avilpage.com/2018/11/comparision-alexa-majestic-domcorp-top-million-sites.html</link><dc:creator>Chillar Anand</dc:creator><description>&lt;h4&gt;Introduction&lt;/h4&gt;
&lt;p&gt;Alexa&lt;sup id="fnref:alexa"&gt;&lt;a class="footnote-ref" href="https://avilpage.com/2018/11/comparision-alexa-majestic-domcorp-top-million-sites.html#fn:alexa"&gt;1&lt;/a&gt;&lt;/sup&gt;, Domcop&lt;sup id="fnref:domcop"&gt;&lt;a class="footnote-ref" href="https://avilpage.com/2018/11/comparision-alexa-majestic-domcorp-top-million-sites.html#fn:domcop"&gt;2&lt;/a&gt;&lt;/sup&gt;(based on CommonCrawl&lt;sup id="fnref:commoncrawl"&gt;&lt;a class="footnote-ref" href="https://avilpage.com/2018/11/comparision-alexa-majestic-domcorp-top-million-sites.html#fn:commoncrawl"&gt;3&lt;/a&gt;&lt;/sup&gt; data) Majestic&lt;sup id="fnref:majestic"&gt;&lt;a class="footnote-ref" href="https://avilpage.com/2018/11/comparision-alexa-majestic-domcorp-top-million-sites.html#fn:majestic"&gt;4&lt;/a&gt;&lt;/sup&gt; &amp;amp;  provide top 1 million popular websites based on their analytics. In this article we will download this data and compare them using Linux command line tools.&lt;/p&gt;
&lt;h4&gt;Collecting data&lt;/h4&gt;
&lt;p&gt;Let's download data from above sources and extract domain names. The data format is different for each source. We can use &lt;code&gt;awk&lt;/code&gt; tool to extract domains column from the source. After extracting data, sort it and save it to a file.&lt;/p&gt;
&lt;p&gt;Extracting domains from alexa.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="c1"&gt;# alexa&lt;/span&gt;

$ wget http://s3.amazonaws.com/alexa-static/top-1m.csv.zip

$ unzip top-1m.csv.zip

&lt;span class="c1"&gt;# data sorted by ranking&lt;/span&gt;
$ head -n &lt;span class="m"&gt;5&lt;/span&gt; top-1m.csv
&lt;span class="m"&gt;1&lt;/span&gt;,google.com
&lt;span class="m"&gt;2&lt;/span&gt;,youtube.com
&lt;span class="m"&gt;3&lt;/span&gt;,facebook.com
&lt;span class="m"&gt;4&lt;/span&gt;,baidu.com
&lt;span class="m"&gt;5&lt;/span&gt;,wikipedia.org

$ awk -F &lt;span class="s2"&gt;","&lt;/span&gt; &lt;span class="s1"&gt;'{print $2}'&lt;/span&gt; top-1m.csv &lt;span class="p"&gt;|&lt;/span&gt; sort &amp;gt; alexa

&lt;span class="c1"&gt;# domains after sorting alphabetically&lt;/span&gt;
$ head -n &lt;span class="m"&gt;5&lt;/span&gt; alexa
&lt;span class="m"&gt;00000&lt;/span&gt;.life
&lt;span class="m"&gt;00&lt;/span&gt;-000.pl
&lt;span class="m"&gt;00004&lt;/span&gt;.tel
&lt;span class="m"&gt;00008888&lt;/span&gt;.tumblr.com
0002rick.tumblr.com
&lt;/pre&gt;
&lt;p&gt;Extracting domain names from domcop.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="c1"&gt;# Domcop&lt;/span&gt;

$ wget https://www.domcop.com/files/top/top10milliondomains.csv.zip

$ unzip top10milliondomains.csv.zip

&lt;span class="c1"&gt;# data sorted by ranking&lt;/span&gt;
$ head -n &lt;span class="m"&gt;5&lt;/span&gt; top10milliondomains.csv
&lt;span class="s2"&gt;"Rank"&lt;/span&gt;,&lt;span class="s2"&gt;"Domain"&lt;/span&gt;,&lt;span class="s2"&gt;"Open Page Rank"&lt;/span&gt;
&lt;span class="s2"&gt;"1"&lt;/span&gt;,&lt;span class="s2"&gt;"fonts.googleapis.com"&lt;/span&gt;,&lt;span class="s2"&gt;"10.00"&lt;/span&gt;
&lt;span class="s2"&gt;"2"&lt;/span&gt;,&lt;span class="s2"&gt;"facebook.com"&lt;/span&gt;,&lt;span class="s2"&gt;"10.00"&lt;/span&gt;
&lt;span class="s2"&gt;"3"&lt;/span&gt;,&lt;span class="s2"&gt;"youtube.com"&lt;/span&gt;,&lt;span class="s2"&gt;"10.00"&lt;/span&gt;
&lt;span class="s2"&gt;"4"&lt;/span&gt;,&lt;span class="s2"&gt;"twitter.com"&lt;/span&gt;,&lt;span class="s2"&gt;"10.00"&lt;/span&gt;

$ awk -F &lt;span class="s2"&gt;"\"*,\"*"&lt;/span&gt; &lt;span class="s1"&gt;'{if(NR&amp;gt;1)print $2}'&lt;/span&gt; top10milliondomains.csv.zip &lt;span class="p"&gt;|&lt;/span&gt; sort &amp;gt; domcop

&lt;span class="c1"&gt;# domains after sorting alphabetically&lt;/span&gt;
$ head -n &lt;span class="m"&gt;5&lt;/span&gt; domcop
00000000b.com
000000book.com
&lt;span class="m"&gt;0000180&lt;/span&gt;.fortunecity.com
&lt;span class="m"&gt;000139418&lt;/span&gt;.wixsite.com
000fashions.blogspot.com
&lt;/pre&gt;
&lt;p&gt;Extracting domain names from majestic.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="c1"&gt;# Majestic&lt;/span&gt;

$ wget http://downloads.majestic.com/majestic_million.csv

&lt;span class="c1"&gt;# data sorted by ranking&lt;/span&gt;
$ head -n &lt;span class="m"&gt;5&lt;/span&gt; majestic_million.csv
GlobalRank,TldRank,Domain,TLD,RefSubNets,RefIPs,IDN_Domain,IDN_TLD,PrevGlobalRank,PrevTldRank,PrevRefSubNets,PrevRefIPs
&lt;span class="m"&gt;1&lt;/span&gt;,1,google.com,com,474277,3016409,google.com,com,1,1,474577,3012875
&lt;span class="m"&gt;2&lt;/span&gt;,2,facebook.com,com,462854,3093315,facebook.com,com,2,2,462860,3090006
&lt;span class="m"&gt;3&lt;/span&gt;,3,youtube.com,com,422434,2504924,youtube.com,com,3,3,422377,2501555
&lt;span class="m"&gt;4&lt;/span&gt;,4,twitter.com,com,412950,2497935,twitter.com,com,4,4,413220,2495261

$ awk -F &lt;span class="s2"&gt;"\"*,\"*"&lt;/span&gt; &lt;span class="s1"&gt;'{if(NR&amp;gt;1)print $2}'&lt;/span&gt; majestic_million.csv &lt;span class="p"&gt;|&lt;/span&gt; sort &amp;gt; majestic

&lt;span class="c1"&gt;# domains after sorting alphabetically&lt;/span&gt;
$ head -n &lt;span class="m"&gt;5&lt;/span&gt; majestic
&lt;span class="m"&gt;00000&lt;/span&gt;.xn--p1ai
&lt;span class="m"&gt;0000666&lt;/span&gt;.com
&lt;span class="m"&gt;0000&lt;/span&gt;.jp
0000www.com
&lt;span class="m"&gt;0000&lt;/span&gt;.xn--p1ai
&lt;/pre&gt;
&lt;h4&gt;Comparing Data&lt;/h4&gt;
&lt;p&gt;We have collected and extracted domains from above sources. Let's compare the domains to see how similar they are using &lt;code&gt;comm&lt;/code&gt; tool.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;$ comm -123 alexa domcop --total
&lt;span class="m"&gt;871851&lt;/span&gt;  &lt;span class="m"&gt;871851&lt;/span&gt;  &lt;span class="m"&gt;128149&lt;/span&gt;  total

$ comm -123 alexa majestic --total
&lt;span class="m"&gt;788454&lt;/span&gt;  &lt;span class="m"&gt;788454&lt;/span&gt;  &lt;span class="m"&gt;211546&lt;/span&gt;  total

$ comm -123 domcop majestic --total
&lt;span class="m"&gt;784388&lt;/span&gt;  &lt;span class="m"&gt;784388&lt;/span&gt;  &lt;span class="m"&gt;215612&lt;/span&gt;  total
&lt;/pre&gt;
&lt;pre class="code literal-block"&gt;$ comm -12 alexa domcop &lt;span class="p"&gt;|&lt;/span&gt; comm -123 - majestic --total
&lt;span class="m"&gt;31314&lt;/span&gt;   &lt;span class="m"&gt;903165&lt;/span&gt;  &lt;span class="m"&gt;96835&lt;/span&gt;   total
&lt;/pre&gt;
&lt;p&gt;So, only 96,835(9.6%) domains are common between all the datasets and the overlap between any two sources is ~20%. Here is a venn diagram showing the overlap between them.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://avilpage.com/images/million-alexa-majestic-domcop.png"&gt;
&lt;/p&gt;

&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;We have collected data from alexa, domcorp &amp;amp; majestic, extracted domains from it and observed that there is only a small overlap between them.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:alexa"&gt;
&lt;p&gt;&lt;a href="http://s3.amazonaws.com/alexa-static/top-1m.csv.zip"&gt;http://s3.amazonaws.com/alexa-static/top-1m.csv.zip&lt;/a&gt; &lt;a class="footnote-backref" href="https://avilpage.com/2018/11/comparision-alexa-majestic-domcorp-top-million-sites.html#fnref:alexa" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:domcop"&gt;
&lt;p&gt;&lt;a href="https://www.domcop.com/files/top/top10milliondomains.csv.zip"&gt;https://www.domcop.com/files/top/top10milliondomains.csv.zip&lt;/a&gt; &lt;a class="footnote-backref" href="https://avilpage.com/2018/11/comparision-alexa-majestic-domcorp-top-million-sites.html#fnref:domcop" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:commoncrawl"&gt;
&lt;p&gt;&lt;a href="https://commoncrawl.org/"&gt;https://commoncrawl.org/&lt;/a&gt; &lt;a class="footnote-backref" href="https://avilpage.com/2018/11/comparision-alexa-majestic-domcorp-top-million-sites.html#fnref:commoncrawl" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:majestic"&gt;
&lt;p&gt;&lt;a href="http://downloads.majestic.com/majestic_million.csv"&gt;http://downloads.majestic.com/majestic_million.csv&lt;/a&gt; &lt;a class="footnote-backref" href="https://avilpage.com/2018/11/comparision-alexa-majestic-domcorp-top-million-sites.html#fnref:majestic" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>command-line</category><category>data-analysis</category><category>python</category><guid>https://avilpage.com/2018/11/comparision-alexa-majestic-domcorp-top-million-sites.html</guid><pubDate>Fri, 02 Nov 2018 06:34:58 GMT</pubDate></item><item><title>Linux Performance Analysis In Less Than 10 Seconds</title><link>https://avilpage.com/2018/07/linux-performance-analysis-in-seconds.html</link><dc:creator>Chillar Anand</dc:creator><description>&lt;p&gt;If you are using a Linux System or managing a Linux server, you might come across a situation where a process is taking too long to complete. In this article we will see how to track down such performance issues in Linux.&lt;/p&gt;
&lt;p&gt;Netflix TechBlog has an article on how to &lt;a href="https://medium.com/netflix-techblog/linux-performance-analysis-in-60-000-milliseconds-accc10403c55"&gt;anlyze Linux performance in 60 seconds&lt;/a&gt;. This article  provides 10+ tools to use in order to see the resource usage and pinpoint the bottleneck.&lt;/p&gt;
&lt;p&gt;It is strenuous to remember all those tools/options and laborious to run all those commands when working on multiple systems.&lt;/p&gt;
&lt;p&gt;Instead, we can use &lt;a href="http://atoptool.nl/"&gt;atop&lt;/a&gt;, a tool for one stop solution for performance analysis. Here is a comparision of atop with other tools from &lt;a href="https://lwn.net/Articles/387202/"&gt;LWN&lt;/a&gt;.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://avilpage.com/images/linux-performance-analysis-tools.png" height="300px" width="600"&gt;
&lt;/p&gt;

&lt;p&gt;atop shows live &amp;amp; historical data measurement at system level as well as process level. To get the glimpse of system resource(CPU, memory, network, disk) usage install and run atop with&lt;/p&gt;
&lt;pre class="code literal-block"&gt;$ sudo apt install --yes atop

$ atop
&lt;/pre&gt;
&lt;p align="center"&gt;
&lt;img src="https://avilpage.com/images/atop.png" height="300px" width="600"&gt;
&lt;/p&gt;

&lt;p&gt;By default, atop shows resources used in the last interval only and sorts them by CPU usage. We can use&lt;/p&gt;
&lt;pre class="code literal-block"&gt;$ atop -A -f &lt;span class="m"&gt;4&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;-A&lt;/code&gt; sorts the processes automatically in the order of the most busy system resource.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-f&lt;/code&gt; shows both active as well as inactive system resources in the ouput.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;4&lt;/code&gt; sets refresh interval to 4 seconds.&lt;/p&gt;
&lt;p&gt;Just by looking at the output of atop, we get a glimpse of overall system resource usage as well as individual processes resource usage.&lt;/p&gt;</description><category>command-line</category><category>linux</category><guid>https://avilpage.com/2018/07/linux-performance-analysis-in-seconds.html</guid><pubDate>Tue, 24 Jul 2018 15:51:21 GMT</pubDate></item><item><title>Bluetooth Serial Communication Between Linux &amp; Android</title><link>https://avilpage.com/2017/10/bluetooth-communication-between-ubuntu-android.html</link><dc:creator>Chillar Anand</dc:creator><description>&lt;p&gt;Most laptops and smart phones(Android/iPhone) have builtin Bluetooth modules. We can use this bluetooth module to communicate with other devices or with other bluetooth modules like HC-05 or HM-10.&lt;/p&gt;
&lt;p&gt;In this article, we will learn how to send data between laptop and android bluetooth.&lt;/p&gt;
&lt;p&gt;First, we need to pair with a bluetooth device to send information. From Ubuntu, we can pair to a Bluetooth device from Bluetooth settings. Alternatively, we can also use CLI to do the same.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;bluetoothctl&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;NEW&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Controller&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="nl"&gt;A&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="nl"&gt;D7&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;99&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="n"&gt;AC&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;asus&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;NEW&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Device&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;94&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="nl"&gt;E9&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;79&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="nl"&gt;BB&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nl"&gt;F8&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;DESKTOP&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;C4ECO3K&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;NEW&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Device&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;88&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;79&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="nl"&gt;E&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="nl"&gt;B&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="nl"&gt;C&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;87&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;athene&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;NEW&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Device&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;94&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;65&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="nl"&gt;D&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="nl"&gt;C&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="nl"&gt;E&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;OnePlus&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;NEW&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Device&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;98&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="nl"&gt;C&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nl"&gt;A5&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;61&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="nl"&gt;D5&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Lenovo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;VIBE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;K5&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Plus&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;NEW&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Device&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;AC&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nl"&gt;C3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="nl"&gt;A&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nl"&gt;A0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nl"&gt;CE&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;EF&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Galaxy&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;J2&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;NEW&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Device&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;98&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="nl"&gt;D3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;35&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;71&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;02&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="n"&gt;B3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;HC&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;05&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;bluetooth&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;power&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;on&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;Changing&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;power&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;on&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;succeeded&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;bluetooth&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;on&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;Agent&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;registered&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;bluetooth&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;Default&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;successful&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;bluetooth&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;scan&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;on&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;Discovery&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;started&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;CHG&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Controller&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="nl"&gt;A&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="nl"&gt;D7&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;99&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="n"&gt;AC&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;Discovering&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;yes&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;CHG&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Device&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;94&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="nl"&gt;E9&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;79&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="nl"&gt;BB&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nl"&gt;F8&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;RSSI&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;88&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;CHG&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Device&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;88&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;79&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="nl"&gt;E&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="nl"&gt;B&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="nl"&gt;C&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;87&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;RSSI&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;66&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;bluetooth&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;pair&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;88&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;79&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="nl"&gt;E&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="nl"&gt;B&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="nl"&gt;C&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;87&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;Attempting&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;to&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;pair&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;88&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;79&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="nl"&gt;E&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="nl"&gt;B&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="nl"&gt;C&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;87&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;CHG&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Device&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;88&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;79&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="nl"&gt;E&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="nl"&gt;B&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="nl"&gt;C&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;87&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;Paired&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;yes&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="n"&gt;Pairing&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;successful&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;To communicate with paired devices, we will use &lt;a href="https://en.wikipedia.org/wiki/List_of_Bluetooth_protocols"&gt;RFCOMM protocol&lt;/a&gt;. RFCOMM is just a serial port emulation and provides reliable data tranfer like TCP.&lt;/p&gt;
&lt;p&gt;From ubuntu, lets open a port for communication.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;$ sudo rfcomm listen /dev/rfcomm0 &lt;span class="m"&gt;3&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;From Android, we have to connect to ubuntu. For this, we can use &lt;a href="https://play.google.com/store/apps/details?id=com.hardcodedjoy.roboremofree&amp;amp;hl=en"&gt;Roboremo&lt;/a&gt; app which supports RFCOMM.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;$ sudo rfcomm listen /dev/rfcomm0 &lt;span class="m"&gt;3&lt;/span&gt;
Waiting &lt;span class="k"&gt;for&lt;/span&gt; connection on channel &lt;span class="m"&gt;3&lt;/span&gt;
Connection from &lt;span class="m"&gt;88&lt;/span&gt;:79:7E:7B:4C:87 to /dev/rfcomm0
Press CTRL-C &lt;span class="k"&gt;for&lt;/span&gt; hangup
&lt;/pre&gt;
&lt;p&gt;Once the connection is established, we can communicate between devices.&lt;/p&gt;
&lt;p&gt;In Unix like systems, OS provides a device file as an interface for device driver. To send and read messages from Linux or Mac is as easy as reading and writing to a file.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="c1"&gt;# to send message to bluetooth&lt;/span&gt;
$ &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s1"&gt;'hello from ubuntu'&lt;/span&gt; &amp;gt; /dev/rfcomm0
&lt;/pre&gt;
&lt;p&gt;We can see the received messages on Android&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="https://avilpage.com/images/arduino-ubuntu-bluetooth.jpg"&gt;
&lt;/p&gt;

&lt;p&gt;We can also send messages from android and read from ubuntu.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="c1"&gt;# to read messages from bluetooth&lt;/span&gt;
$ cat /dev/rfcomm0
hello from android
&lt;/pre&gt;
&lt;p&gt;This way, we can communicate with any bluetooth module using a laptop or a smart phone.&lt;/p&gt;</description><category>android</category><category>bluetooth</category><category>command-line</category><category>linux</category><guid>https://avilpage.com/2017/10/bluetooth-communication-between-ubuntu-android.html</guid><pubDate>Tue, 03 Oct 2017 14:53:04 GMT</pubDate></item><item><title>Arduino Programming From Text Editor &amp; Command Line</title><link>https://avilpage.com/2017/08/arduino-programming-from-text-editor-cli.html</link><dc:creator>Chillar Anand</dc:creator><description>&lt;p&gt;To program Arduino, we can use &lt;a href="https://www.arduino.cc/en/main/software"&gt;Arduino IDE&lt;/a&gt; which facilitates writing and uploading code to the board.&lt;/p&gt;
&lt;p&gt;If we are using a text editor for programming, it will have lot of customisations which speed up development process. In such case, it is better to use same text editor for arduino programming too.&lt;/p&gt;
&lt;p&gt;I use Emacs as IDE and there is &lt;a href="https://github.com/bookest/arduino-mode"&gt;arduino mode&lt;/a&gt; for emacs which provides syntax highlighting and some useful utilites to write arduino code. We can find such packages for other editors also.&lt;/p&gt;
&lt;p&gt;Arduino also provides cli interface to upload code to arduino. To upload code, we need to specify port, board and the code to upload.&lt;/p&gt;
&lt;p&gt;In Linux system, to upload a file called &lt;code&gt;foo.ino&lt;/code&gt;, we can run&lt;/p&gt;
&lt;pre class="code literal-block"&gt;arduino --port /dev/ttyACM0 --board arduino:avr:mega
 &lt;span class="se"&gt;\ &lt;/span&gt;--upload foo.ino
&lt;/pre&gt;
&lt;p&gt;An alternate way is to use &lt;a href="https://github.com/platformio/platformio-core/"&gt;platformio&lt;/a&gt;, an opensource tool chain for IoT development.&lt;/p&gt;
&lt;p&gt;It can be installed using pip.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip install platformio
&lt;/pre&gt;
&lt;p&gt;Once it is installed, code can be directly uploaded using &lt;code&gt;ci&lt;/code&gt; command.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pio ci --board&lt;span class="o"&gt;=&lt;/span&gt;megaatmega2560 --project-option&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"upload_port=/dev/ttyACM0"&lt;/span&gt; --project-option&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"targets=upload"&lt;/span&gt; foo.ino
&lt;/pre&gt;
&lt;p&gt;By this we can use text editor to write code and arduino/platformio to upload code to arduino board.&lt;/p&gt;</description><category>arduino</category><category>command-line</category><guid>https://avilpage.com/2017/08/arduino-programming-from-text-editor-cli.html</guid><pubDate>Thu, 24 Aug 2017 13:29:59 GMT</pubDate></item><item><title>Super Charge Your Shell For Python Development</title><link>https://avilpage.com/2017/03/super-charge-your-shell-for-python-development.html</link><dc:creator>Chillar Anand</dc:creator><description>&lt;p&gt;Last month, I gave a lightning talk about supercharging your shell for python development at &lt;a href="http://www.meetup.com/BangPypers/"&gt;BangPypers meetup&lt;/a&gt;.&lt;/p&gt;
&lt;iframe width="600" height="350" src="https://www.youtube.com/embed/lvmJ0tWCjFA" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;This is a detailed blog post on how to setup your laptop for the same.&lt;/p&gt;
&lt;h3&gt;Autojump&lt;/h3&gt;
&lt;p&gt;When working on terminal, &lt;code&gt;cd&lt;/code&gt; is used to traverse directories.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; ~/projects/python/django
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;cd&lt;/code&gt; is inefficient to quickly traverse directories which are in different paths and far away from each other.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; /var/lib/elasticsearch/
&lt;span class="nb"&gt;cd&lt;/span&gt; ~/sandbox/channels/demo
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;z&lt;/code&gt;, a oh-my-zsh plugin is efficient for traversing directories. With &lt;code&gt;z&lt;/code&gt;, directory can be changed by typing name of directory.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;z junction
&lt;/pre&gt;
&lt;p&gt;Instead of full name, just a substring would do.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;z ju
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;z&lt;/code&gt; keeps a score of all visited directories and moves to most frecency(frequent+recent) directory that matches the substring.&lt;/p&gt;
&lt;p&gt;To install &lt;code&gt;z&lt;/code&gt;, install &lt;a href="https://avilpage.com/2015/03/install-oh-my-zsh-on-ubuntu.html"&gt;oh-my-zsh&lt;/a&gt; and add &lt;code&gt;z&lt;/code&gt; to plugins in &lt;code&gt;.zshrc&lt;/code&gt; file.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="nv"&gt;plugins&lt;/span&gt;&lt;span class="o"&gt;=(&lt;/span&gt;git z&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;h3&gt;Aliases&lt;/h3&gt;
&lt;p&gt;Read this old blog post on how &lt;a href="https://avilpage.com/2014/10/useful-shell-aliases-for-python-and.html"&gt;aliases will improve your productivity&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Autoenv&lt;/h3&gt;
&lt;p&gt;When working on multiple projects, it becomes necessary to use virtualenvs so that multiple versions of same package can be used. In addition to that, it be necessary to set environment variables on a per project basis.&lt;/p&gt;
&lt;p&gt;To automate all these things, &lt;a href="https://pypi.python.org/pypi/autoenv/"&gt;autoenv&lt;/a&gt; provides directory based environments. Whenever user changes directory, it will help to automatically activate environment and set environment variables.&lt;/p&gt;
&lt;p&gt;If you have file named &lt;code&gt;.env&lt;/code&gt; in a directory, autoenv will automatically source that file whenever user enters into it.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;autoenv&lt;/code&gt; is a python package. It can be installed with&lt;/p&gt;
&lt;pre class="code literal-block"&gt;pip install autoenv
&lt;/pre&gt;
&lt;p&gt;It provides a shell script which needs to sourced.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;"source `which activate.sh`"&lt;/span&gt; &amp;gt;&amp;gt; ~/.bashrc
&lt;/pre&gt;
&lt;p&gt;You can create a .env file like this in project root.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="nb"&gt;source&lt;/span&gt; ~/.virtualenvs/exp/bin/activate
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;SECRET_KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;foobar
&lt;/pre&gt;
&lt;p&gt;Next time, when your enter into that directory, &lt;code&gt;autoenv&lt;/code&gt; finds &lt;code&gt;.env&lt;/code&gt; file and it will source it automatically.&lt;/p&gt;
&lt;h3&gt;Autoreload&lt;/h3&gt;
&lt;p&gt;I have written a sepeate blog post on how to &lt;a href="https://avilpage.com/2014/11/python-automagically-reload-imports-in.html"&gt;automagically reload imports&lt;/a&gt; long time back.&lt;/p&gt;
&lt;h3&gt;Autoimports&lt;/h3&gt;
&lt;p&gt;When you copy code and paste it in ipython interpreter, it might fail with &lt;code&gt;ImportError&lt;/code&gt; if required modules aren't already imported by the interpreter.&lt;/p&gt;
&lt;p&gt;Also when playing with code, having some predefined data would be handy. This avoids populating of data everytime shell starts.&lt;/p&gt;
&lt;p&gt;You can write an init script which will do all these things and load it automatically when ipython starts.&lt;/p&gt;
&lt;p&gt;Here is a &lt;a href="https://github.com/ChillarAnand/01/blob/master/python/ipython_config.py"&gt;simple init script&lt;/a&gt; which I use to auto import modules and data. This file can be auto loaded by specifying it in your config file.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;InteractiveShellApp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exec_files&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;directory&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'ipython_init.py'&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;
&lt;h3&gt;Autocall&lt;/h3&gt;
&lt;p&gt;When using python interpreter, to call a function, you have to type parenthesis.Typing parenthesis is not ergonomic as you have to move both hands far away from homerow.&lt;/p&gt;
&lt;p&gt;IPython provides &lt;code&gt;autocall&lt;/code&gt; option to make functions callable without typing parenthesis. This can be activate with &lt;code&gt;%autocall&lt;/code&gt; magic.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;autocall&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;Automatic&lt;/span&gt; &lt;span class="n"&gt;calling&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Smart&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Now functions can be called without parenthesis.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="o"&gt;------&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;You can also enable this by default by activating it in ipython config file.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;InteractiveShellApp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exec_lines&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="si"&gt;%a&lt;/span&gt;&lt;span class="s1"&gt;utocall  1'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;These are some tips to become more productive with your shell when working on python projects.&lt;/p&gt;</description><category>command-line</category><category>python</category><guid>https://avilpage.com/2017/03/super-charge-your-shell-for-python-development.html</guid><pubDate>Sun, 26 Mar 2017 02:40:31 GMT</pubDate></item><item><title>Concurrent Downloads - Bash Vs Python</title><link>https://avilpage.com/2016/05/concurrent-downloads-bash-vs-python.html</link><dc:creator>Chillar Anand</dc:creator><description>&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                I just found one more free telugu book &lt;code style="background-color: rgba(0, 0, 0, 0.0392157); border-radius: 3px; box-sizing: border-box; font-family: Consolas, 'Liberation Mono', Menlo, Courier, monospace; font-size: 13.6px; margin: 0px; padding: 0.2em 0px;"&gt;Graded
      readings in modern literary Telugu&lt;/code&gt; by Golla Narayanaswami Reddy and Dan M Matson in &lt;a href="http://dsal.uchicago.edu/digbooks/dig_toc.html?BOOKID=PL4775.R4_1967" style="box-sizing: border-box; color: #4078c0; cursor: pointer; text-decoration: none;"&gt;Digital South Asia Library&lt;/a&gt;.
            &lt;/div&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                Unfortunately they didn't provide it as an ebook but as a set of 221 tif images.
            &lt;/div&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                I wrote a simple for loop in shell which downloaded all images one by one using &lt;code style="background-color: rgba(0, 0, 0, 0.0392157); border-radius: 3px; box-sizing: border-box; font-family: Consolas, 'Liberation Mono', Menlo, Courier, monospace; font-size: 13.6px; margin: 0px; padding: 0.2em 0px;"&gt;wget&lt;/code&gt;.
            &lt;/div&gt;
            &lt;div class="highlight highlight-source-shell" style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                &lt;pre style="background-color: #f7f7f7; border-radius: 3px; box-sizing: border-box; font-family: Consolas, 'Liberation Mono', Menlo, Courier, monospace; font-size: 13.6px; font-stretch: normal; line-height: 1.45; overflow: auto; padding: 16px; word-break: normal; word-wrap: normal;"&gt;
      $ base_url=&lt;span class="pl-s" style="box-sizing: border-box; color: #183691;"&gt;&lt;span class="pl-pds" style="box-sizing: border-box;"&gt;"&lt;/span&gt;http://dsal.uchicago.edu&lt;span class="pl-pds" style="box-sizing: border-box;"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;br&gt;$ url=&lt;span class="pl-s" style="box-sizing: border-box; color: #183691;"&gt;&lt;span class="pl-pds" style="box-sizing: border-box;"&gt;"&lt;/span&gt;&lt;span class="pl-smi" style="box-sizing: border-box; color: #333333;"&gt;$base_url&lt;/span&gt;/digbooks/images/PL4775.R4_1967/PL4775.R4_1967_%03g.gif&lt;span class="pl-pds" style="box-sizing: border-box;"&gt;"&lt;/span&gt;&lt;/span&gt;&lt;br&gt;$ &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;time&lt;/span&gt; -p sh -c &lt;span class="pl-s" style="box-sizing: border-box; color: #183691;"&gt;&lt;span class="pl-pds" style="box-sizing: border-box;"&gt;'&lt;/span&gt;for i in $(seq -f $url 1 221); do; wget $i; done;&lt;span class="pl-pds" style="box-sizing: border-box;"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;
            &lt;/div&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                I took 375 seconds for that. This was too slow. So I tried to download them parallelly using xargs.
            &lt;/div&gt;
            &lt;div class="highlight highlight-source-shell" style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                &lt;pre style="background-color: #f7f7f7; border-radius: 3px; box-sizing: border-box; font-family: Consolas, 'Liberation Mono', Menlo, Courier, monospace; font-size: 13.6px; font-stretch: normal; line-height: 1.45; overflow: auto; padding: 16px; word-break: normal; word-wrap: normal;"&gt;
      $ &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;time&lt;/span&gt; &lt;span class="pl-c1" style="box-sizing: border-box; color: #0086b3;"&gt;echo&lt;/span&gt; &lt;span class="pl-s" style="box-sizing: border-box; color: #183691;"&gt;&lt;span class="pl-pds" style="box-sizing: border-box;"&gt;$(&lt;/span&gt;seq -f &lt;span class="pl-smi" style="box-sizing: border-box; color: #333333;"&gt;$url&lt;/span&gt; 1 221&lt;span class="pl-pds" style="box-sizing: border-box;"&gt;)&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;|&lt;/span&gt; xargs -n 1 -P 36 wget&lt;/pre&gt;
            &lt;/div&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                My laptop has a quad core processor. So I tried with 20, 24, 28, 32 process at a time.
            &lt;/div&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                With &lt;code style="background-color: rgba(0, 0, 0, 0.0392157); border-radius: 3px; box-sizing: border-box; font-family: Consolas, 'Liberation Mono', Menlo, Courier, monospace; font-size: 13.6px; margin: 0px; padding: 0.2em 0px;"&gt;wget+xargs&lt;/code&gt;,
                the best timing is 13 seconds (CPU: 15%, Process: 28).
            &lt;/div&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                Again I tried downloading them parallelly but with GNU parallel.
            &lt;/div&gt;
            &lt;div class="highlight highlight-source-shell" style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                &lt;pre style="background-color: #f7f7f7; border-radius: 3px; box-sizing: border-box; font-family: Consolas, 'Liberation Mono', Menlo, Courier, monospace; font-size: 13.6px; font-stretch: normal; line-height: 1.45; overflow: auto; padding: 16px; word-break: normal; word-wrap: normal;"&gt;
      $ &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;time&lt;/span&gt; seq -f &lt;span class="pl-smi" style="box-sizing: border-box;"&gt;$url&lt;/span&gt; 1 221 &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;|&lt;/span&gt; parallel -j36 wget {}&lt;/pre&gt;
            &lt;/div&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                With &lt;code style="background-color: rgba(0, 0, 0, 0.0392157); border-radius: 3px; box-sizing: border-box; font-family: Consolas, 'Liberation Mono', Menlo, Courier, monospace; font-size: 13.6px; margin: 0px; padding: 0.2em 0px;"&gt;wget+parallel&lt;/code&gt;,
                the best timing is 12 seconds (CPU: 48%, Process: 24).
            &lt;/div&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                Here is cpu consumption and time taken at each step.
            &lt;/div&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                &lt;a href="https://cloud.githubusercontent.com/assets/4463796/15634659/08965d50-25e7-11e6-9f3e-02fe008997a1.png" style="box-sizing: border-box; color: #4078c0; cursor: pointer; text-decoration: none;" target="_blank"&gt;&lt;img alt="paralle_python_bash2" src="https://cloud.githubusercontent.com/assets/4463796/15634659/08965d50-25e7-11e6-9f3e-02fe008997a1.png" style="border: 0px; box-sizing: border-box; max-width: 100%;"&gt;&lt;/a&gt;
            &lt;/div&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                Once I have done with bash, I decided to try the same things with Python and see how it goes.
            &lt;/div&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                I wrote a simple script using requests to download images.
            &lt;/div&gt;
            &lt;div class="highlight highlight-source-python" style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                &lt;pre style="background-color: #f7f7f7; border-radius: 3px; box-sizing: border-box; font-family: Consolas, 'Liberation Mono', Menlo, Courier, monospace; font-size: 13.6px; font-stretch: normal; line-height: 1.45; overflow: auto; padding: 16px; word-break: normal; word-wrap: normal;"&gt;
                    &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;import&lt;/span&gt; shutil&lt;br&gt;&lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;import&lt;/span&gt; sys&lt;br&gt;&lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;from&lt;/span&gt; concurrent &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;import&lt;/span&gt; futures&lt;br&gt;&lt;br&gt;&lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;import&lt;/span&gt; requests&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;def&lt;/span&gt; &lt;span class="pl-en" style="box-sizing: border-box; color: #795da3;"&gt;download_image&lt;/span&gt;(&lt;span class="pl-smi" style="box-sizing: border-box;"&gt;url&lt;/span&gt;):&lt;br&gt;    r &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;=&lt;/span&gt; requests.get(url)&lt;br&gt;    file_name &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;=&lt;/span&gt; url.split(&lt;span class="pl-s" style="box-sizing: border-box; color: #183691;"&gt;&lt;span class="pl-pds" style="box-sizing: border-box;"&gt;'&lt;/span&gt;/&lt;span class="pl-pds" style="box-sizing: border-box;"&gt;'&lt;/span&gt;&lt;/span&gt;)[&lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;-&lt;/span&gt;&lt;span class="pl-c1" style="box-sizing: border-box; color: #0086b3;"&gt;1&lt;/span&gt;]&lt;br&gt;    &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;with&lt;/span&gt; &lt;span class="pl-c1" style="box-sizing: border-box; color: #0086b3;"&gt;open&lt;/span&gt;(file_name, &lt;span class="pl-s" style="box-sizing: border-box; color: #183691;"&gt;&lt;span class="pl-pds" style="box-sizing: border-box;"&gt;'&lt;/span&gt;wb&lt;span class="pl-pds" style="box-sizing: border-box;"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;as&lt;/span&gt; fh:&lt;br&gt;        fh.write(r.content)&lt;br&gt;&lt;br&gt;&lt;br&gt;base_url &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;=&lt;/span&gt; &lt;span class="pl-s" style="box-sizing: border-box; color: #183691;"&gt;&lt;span class="pl-pds" style="box-sizing: border-box;"&gt;'&lt;/span&gt;http://dsal.uchicago.edu&lt;span class="pl-pds" style="box-sizing: border-box;"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;br&gt;book_url &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;=&lt;/span&gt; base_url &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;+&lt;/span&gt; &lt;span class="pl-s" style="box-sizing: border-box; color: #183691;"&gt;&lt;span class="pl-pds" style="box-sizing: border-box;"&gt;'&lt;/span&gt;/digbooks/images/PL4775.R4_1967/PL4775.R4_1967_&lt;span class="pl-c1" style="box-sizing: border-box; color: #0086b3;"&gt;{}&lt;/span&gt;.gif&lt;span class="pl-pds" style="box-sizing: border-box;"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;br&gt;urls &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;=&lt;/span&gt; [book_url.format(&lt;span class="pl-c1" style="box-sizing: border-box; color: #0086b3;"&gt;str&lt;/span&gt;(i).zfill(&lt;span class="pl-c1" style="box-sizing: border-box; color: #0086b3;"&gt;3&lt;/span&gt;)) &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;for&lt;/span&gt; i &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;in&lt;/span&gt; &lt;span class="pl-c1" style="box-sizing: border-box; color: #0086b3;"&gt;range&lt;/span&gt;(&lt;span class="pl-c1" style="box-sizing: border-box; color: #0086b3;"&gt;1&lt;/span&gt;, &lt;span class="pl-c1" style="box-sizing: border-box; color: #0086b3;"&gt;221&lt;/span&gt;)]&lt;br&gt;&lt;br&gt;&lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;def&lt;/span&gt; &lt;span class="pl-en" style="box-sizing: border-box; color: #795da3;"&gt;download_serially&lt;/span&gt;():&lt;br&gt;    &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;for&lt;/span&gt; url &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;in&lt;/span&gt; urls:&lt;br&gt;        download_image(url)&lt;br&gt;&lt;br&gt;download_serially()&lt;/pre&gt;
            &lt;/div&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                This took 244 seconds.
            &lt;/div&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                To download images parallelly, I have used &lt;code style="background-color: rgba(0, 0, 0, 0.0392157); border-radius: 3px; box-sizing: border-box; font-family: Consolas, 'Liberation Mono', Menlo, Courier, monospace; font-size: 13.6px; margin: 0px; padding: 0.2em 0px;"&gt;Threadpoolexecutor&lt;/code&gt; from &lt;a href="https://docs.python.org/3.5/library/concurrent.futures.html" style="box-sizing: border-box; color: #4078c0; cursor: pointer; text-decoration: none;"&gt;concurrent module&lt;/a&gt;.
            &lt;/div&gt;
            &lt;div class="highlight highlight-source-python" style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                &lt;pre style="background-color: #f7f7f7; border-radius: 3px; box-sizing: border-box; font-family: Consolas, 'Liberation Mono', Menlo, Courier, monospace; font-size: 13.6px; font-stretch: normal; line-height: 1.45; overflow: auto; padding: 16px; word-break: normal; word-wrap: normal;"&gt;
                    &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;def&lt;/span&gt; &lt;span class="pl-en" style="box-sizing: border-box; color: #795da3;"&gt;download_parallely&lt;/span&gt;():&lt;br&gt;    workers &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;=&lt;/span&gt; &lt;span class="pl-c1" style="box-sizing: border-box; color: #0086b3;"&gt;int&lt;/span&gt;(sys.argv[&lt;span class="pl-c1" style="box-sizing: border-box; color: #0086b3;"&gt;1&lt;/span&gt;])&lt;br&gt;&lt;br&gt;    &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;with&lt;/span&gt; futures.ThreadPoolExecutor(&lt;span class="pl-v" style="box-sizing: border-box; color: #ed6a43;"&gt;max_workers&lt;/span&gt;&lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;=&lt;/span&gt;workers) &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;as&lt;/span&gt; executor:&lt;br&gt;        result &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;=&lt;/span&gt; executor.map(download_image, urls)&lt;br&gt;&lt;br&gt;download_parallely()&lt;/pre&gt;
            &lt;/div&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                I used previous script but just added one more function which queues tasks. Then I have executed the script with several options.
            &lt;/div&gt;
            &lt;div class="highlight highlight-source-shell" style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                &lt;pre style="background-color: #f7f7f7; border-radius: 3px; box-sizing: border-box; font-family: Consolas, 'Liberation Mono', Menlo, Courier, monospace; font-size: 13.6px; font-stretch: normal; line-height: 1.45; overflow: auto; padding: 16px; word-break: normal; word-wrap: normal;"&gt;
      $ &lt;span class="pl-k" style="box-sizing: border-box; color: #a71d5d;"&gt;time&lt;/span&gt; python down.py 28&lt;/pre&gt;
            &lt;/div&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                Threadpoolexecutor documentation uses 5 times number of processors as max_workers by default. I tried same options which I have used for bash. Here is
                the overall comparision.
            &lt;/div&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                With &lt;code style="background-color: rgba(0, 0, 0, 0.0392157); border-radius: 3px; box-sizing: border-box; font-family: Consolas, 'Liberation Mono', Menlo, Courier, monospace; font-size: 13.6px; margin: 0px; padding: 0.2em 0px;"&gt;requests+ThreadPoolExecutor&lt;/code&gt;,
                the best timing is 12 seconds (CPU: 36%, Process: 28).
            &lt;/div&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                Here is the overall comparision.
            &lt;/div&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                &lt;a href="https://cloud.githubusercontent.com/assets/4463796/15634653/edadbb50-25e6-11e6-8f59-c908ca7d65b2.png" style="box-sizing: border-box; color: #4078c0; cursor: pointer; text-decoration: none;" target="_blank"&gt;&lt;img alt="paralle_python_bash" src="https://cloud.githubusercontent.com/assets/4463796/15634653/edadbb50-25e6-11e6-8f59-c908ca7d65b2.png" style="border: 0px; box-sizing: border-box; max-width: 100%;"&gt;&lt;/a&gt;
            &lt;/div&gt;
            &lt;div style="box-sizing: border-box; color: #333333; font-family: 'Helvetica Neue', Helvetica, 'Segoe UI', Arial, freesans, sans-serif; font-size: 16px; line-height: 25.6px; margin-bottom: 16px;"&gt;
                &lt;span style="line-height: 25.6px;"&gt;For a simple concurrent download, xargs+wget seems to be the best option.&lt;/span&gt;
            &lt;/div&gt;
        &lt;/div&gt;</description><category>command-line</category><category>python</category><guid>https://avilpage.com/2016/05/concurrent-downloads-bash-vs-python.html</guid><pubDate>Sun, 29 May 2016 11:55:00 GMT</pubDate></item><item><title>WD My Cloud NAS Setup On Linux</title><link>https://avilpage.com/2015/03/wd-my-cloud-nas-setup-on-ubuntu.html</link><dc:creator>Chillar Anand</dc:creator><description>&lt;h4&gt;Introduction&lt;/h4&gt;
&lt;p&gt;I recently bought a WD My Cloud NAS&lt;sup id="fnref:nas"&gt;&lt;a class="footnote-ref" href="https://avilpage.com/2015/03/wd-my-cloud-nas-setup-on-ubuntu.html#fn:nas"&gt;1&lt;/a&gt;&lt;/sup&gt; device to store my personal data. I wanted to set it up on my Ubuntu machine. For WD My Cloud, there is no official support for Ubuntu or any other Linux distros. But setting up it is quite easy.&lt;/p&gt;
&lt;h4&gt;NAS Setup&lt;/h4&gt;
&lt;p&gt;Make sure You have connected power adapter &amp;amp; LAN cables to it. If You open Your router config, You will see WD My cloud in client list. Make note of its IP address. If You want, You can assign a static IP also in the router settings.&lt;/p&gt;
&lt;p&gt;Next step is to install NFS client package. NFS(Network File System) allows a system to share directories and files with others over a network. By using NFS, users and programs can access files on remote systems almost as if they were local files. So, update your packages &amp;amp; install &lt;code&gt;nfs-common&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;$ sudo apt-get update
$ sudo apt-get install nfs-common
&lt;/pre&gt;
&lt;p&gt;Now we can list folders which are available to mount using &lt;code&gt;showmount&lt;/code&gt; command.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;$ showmount -e &amp;lt;ip-address&amp;gt;
&lt;/pre&gt;
&lt;p&gt;Create an empty folder to mount any of the folder you wanted and mount it.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;$ sudo mount -o rw,soft,intr,nfsvers&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt; &amp;lt;ip&amp;gt;:&amp;lt;folder-to-mount&amp;gt; &amp;lt;path-to-mount&amp;gt;
&lt;/pre&gt;
&lt;p&gt;Now You can start moving data into/out of WD My Cloud.&lt;/p&gt;
&lt;p&gt;If You want to mount it automatically on boot, add following line to &lt;code&gt;/etc/fstab&lt;/code&gt; file.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&amp;lt;ip&amp;gt;:&amp;lt;folder-to-mount&amp;gt; &amp;lt;path-to-mount&amp;gt; nfs rw,soft,intr,nfsvers&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;
&lt;/pre&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;Even though there is no official support for Ubuntu, WD My Cloud works pretty well with Ubuntu and other Linux distros. &lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:nas"&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Network-attached_storage"&gt;https://en.wikipedia.org/wiki/Network-attached_storage&lt;/a&gt; &lt;a class="footnote-backref" href="https://avilpage.com/2015/03/wd-my-cloud-nas-setup-on-ubuntu.html#fnref:nas" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>command-line</category><category>linux</category><category>ubuntu</category><guid>https://avilpage.com/2015/03/wd-my-cloud-nas-setup-on-ubuntu.html</guid><pubDate>Tue, 17 Mar 2015 08:50:00 GMT</pubDate></item></channel></rss>