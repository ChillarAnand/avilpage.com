<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Avil Page (Posts about health-care)</title><link>http://avilpage.com/</link><description></description><atom:link href="http://avilpage.com/tags/health-care.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sun, 24 Oct 2021 09:55:14 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Mastering Dicom #2 - Setup Orthanc Demo Server</title><link>http://avilpage.com/2021/03/setup-orthanc-demo-server.html</link><dc:creator>Chillar Anand</dc:creator><description>&lt;div&gt;&lt;p&gt;This is a series of &lt;a href="http://avilpage.com/tags/dicom.html"&gt;articles on mastering Dicom&lt;/a&gt;. In the earlier article, we have learnt how PACS/DICOM simplifies the clinical work flow.&lt;/p&gt;
&lt;p&gt;In this article, lets setup a dicom server so that we have a server to play around with Dicom files.&lt;/p&gt;
&lt;h4&gt;Orthanc Server&lt;/h4&gt;
&lt;p&gt;There are several Dicom servers like Orthanc, Dicoogle etc. &lt;a href="https://en.wikipedia.org/wiki/Orthanc_(server)"&gt;Orthanc&lt;/a&gt; is a lightweight open source dicom server and is widely used by many Health care organisations.&lt;/p&gt;
&lt;p&gt;SÃ©bastien Jodogne, original author of Orthanc maintains docker images. We can use these images to run Orthanc server locally.&lt;/p&gt;
&lt;p&gt;Ensure docker is installed on the machine and then run the following command to start Orthanc server.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ docker run -p &lt;span class="m"&gt;4242&lt;/span&gt;:4242 -p &lt;span class="m"&gt;8042&lt;/span&gt;:8042 --rm &lt;span class="se"&gt;\&lt;/span&gt;
    jodogne/orthanc-python
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the server is started, we can visit &lt;a href="http://localhost:8042"&gt;http://localhost:8042&lt;/a&gt; and explore Orthanc server.&lt;/p&gt;
&lt;h4&gt;Heroku Deployment&lt;/h4&gt;
&lt;p&gt;Heroku is PAAS platform which supports docker deployments. Lets deploy Orthac server to Heroku for testing.&lt;/p&gt;
&lt;p&gt;By default, Orthanc server runs on 8042 port as defined in the config file. Heroku dynamically assigns port for the deployed process.&lt;/p&gt;
&lt;p&gt;We can write a shell script which will read port number from environment variable, replace it in Orthanc configuration file and then start Orthanc server.&lt;/p&gt;
&lt;table class="codehilitetable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;
&lt;span class="normal"&gt;9&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="ch"&gt;#! /bin/sh&lt;/span&gt;

&lt;span class="nb"&gt;set&lt;/span&gt; -x

&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="nv"&gt;$PORT&lt;/span&gt;

sed &lt;span class="s1"&gt;'s/ : 8042/ : '&lt;/span&gt;&lt;span class="nv"&gt;$PORT&lt;/span&gt;&lt;span class="s1"&gt;'/g'&lt;/span&gt; -i /etc/orthanc/orthanc.json

Orthanc /etc/orthanc/
&lt;/code&gt;&lt;/pre&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;p&gt;We can use this shell script as entry point in docker as follows.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;FROM jodogne/orthanc-python

EXPOSE $PORT

WORKDIR /app
ADD . /app

ENTRYPOINT [ "./run.sh" ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can create a new app in heroku and we can deploy this container.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ heroku apps:create orthanc-demo

$ heroku container:push web
$ heroku container:release web
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the deployment is completed, we can access our app from the endpoint provided by heroku. Here is a &lt;a href="https://orthanc-demo.herokuapp.com"&gt;orthanc demo server&lt;/a&gt; running on heroku.&lt;/p&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;In this article, we have learnt how to setup Orthanc server and deployed it to Heroku. In the next article, let dig deeper into dicom protocol by upload/accessing dicom files to the server.&lt;/p&gt;&lt;/div&gt;</description><category>dicom</category><category>health-care</category><guid>http://avilpage.com/2021/03/setup-orthanc-demo-server.html</guid><pubDate>Fri, 26 Mar 2021 00:30:00 GMT</pubDate></item><item><title>How To Deploy Mirth Connect To Kubernetes</title><link>http://avilpage.com/2020/07/deploy-mirth-to-kubernetes.html</link><dc:creator>Chillar Anand</dc:creator><description>&lt;div&gt;&lt;h4&gt;Introduction&lt;/h4&gt;
&lt;p&gt;NextGen Connect(previously Mirth Connect) is widely used integration engine for information exchange in health-care domain. In this article, let us see how to deploy Mirth Connect to a Kubernetes cluster.&lt;/p&gt;
&lt;h4&gt;Deployment To k8s&lt;/h4&gt;
&lt;p&gt;From version 3.8, NextGen has started providing official docker images for Connect&lt;sup id="fnref:nc"&gt;&lt;a class="footnote-ref" href="http://avilpage.com/2020/07/deploy-mirth-to-kubernetes.html#fn:nc"&gt;1&lt;/a&gt;&lt;/sup&gt;. By default, Connect docker exposes 8080, 8443 ports. We can start a Connect instance locally, by running the following command.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nv"&gt;$docker&lt;/span&gt; run -p &lt;span class="m"&gt;8080&lt;/span&gt;:8080 -p &lt;span class="m"&gt;8443&lt;/span&gt;:8443 nextgenhealthcare/connect
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can use this docker image and create a k8s deployment to start a container.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;---&lt;/span&gt;
&lt;span class="nv"&gt;apiVersion&lt;/span&gt;: &lt;span class="nv"&gt;apps&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nv"&gt;v1beta1&lt;/span&gt;
&lt;span class="nv"&gt;kind&lt;/span&gt;: &lt;span class="nv"&gt;Deployment&lt;/span&gt;
&lt;span class="nv"&gt;metadata&lt;/span&gt;:
  &lt;span class="nv"&gt;name&lt;/span&gt;: &lt;span class="nv"&gt;mirth&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;connect&lt;/span&gt;
  &lt;span class="nv"&gt;namespace&lt;/span&gt;: &lt;span class="nv"&gt;default&lt;/span&gt;
&lt;span class="nv"&gt;spec&lt;/span&gt;:
  &lt;span class="nv"&gt;template&lt;/span&gt;:
    &lt;span class="nv"&gt;spec&lt;/span&gt;:
      &lt;span class="nv"&gt;containers&lt;/span&gt;:
      &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nv"&gt;name&lt;/span&gt;: &lt;span class="nv"&gt;mirth&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;connect&lt;/span&gt;
        &lt;span class="nv"&gt;image&lt;/span&gt;: &lt;span class="nv"&gt;docker&lt;/span&gt;.&lt;span class="nv"&gt;io&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nv"&gt;nextgenhealthcare&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="k"&gt;connect&lt;/span&gt;
        &lt;span class="nv"&gt;ports&lt;/span&gt;:
        &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nv"&gt;name&lt;/span&gt;: &lt;span class="nv"&gt;http&lt;/span&gt;
          &lt;span class="nv"&gt;containerPort&lt;/span&gt;: &lt;span class="mi"&gt;8080&lt;/span&gt;
        &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nv"&gt;name&lt;/span&gt;: &lt;span class="nv"&gt;https&lt;/span&gt;
          &lt;span class="nv"&gt;containerPort&lt;/span&gt;: &lt;span class="mi"&gt;8443&lt;/span&gt;
        &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nv"&gt;name&lt;/span&gt;: &lt;span class="nv"&gt;hl7&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nv"&gt;test&lt;/span&gt;
          &lt;span class="nv"&gt;containerPort&lt;/span&gt;: &lt;span class="mi"&gt;9001&lt;/span&gt;
        &lt;span class="nv"&gt;env&lt;/span&gt;:
          &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nv"&gt;name&lt;/span&gt;: &lt;span class="nv"&gt;DATABASE&lt;/span&gt;
            &lt;span class="nv"&gt;value&lt;/span&gt;: &lt;span class="nv"&gt;postgres&lt;/span&gt;
          &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nv"&gt;name&lt;/span&gt;: &lt;span class="nv"&gt;DATABASE_URL&lt;/span&gt;
            &lt;span class="nv"&gt;value&lt;/span&gt;: &lt;span class="nv"&gt;jdbc&lt;/span&gt;:&lt;span class="nv"&gt;postgresql&lt;/span&gt;:&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="nv"&gt;avilpage&lt;/span&gt;.&lt;span class="nv"&gt;com&lt;/span&gt;:&lt;span class="mi"&gt;5432&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nv"&gt;mirth_db&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This deployment file can be applied on a cluster using &lt;code&gt;kubectl&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ kubectl apply -f connect-deployment.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To access this container, we can create a service to expose this deployment to public.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;---&lt;/span&gt;
&lt;span class="n"&gt;apiVersion&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;v1&lt;/span&gt;
&lt;span class="n"&gt;kind&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Service&lt;/span&gt;
&lt;span class="n"&gt;metadata&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mirth&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;connect&lt;/span&gt;
  &lt;span class="n"&gt;annotations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;service&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kubernetes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;aws&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nb"&gt;load&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;balancer&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ssl&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;cert&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;arn&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;aws&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;acm&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;ap&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;south&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;foo&lt;/span&gt;
    &lt;span class="n"&gt;service&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kubernetes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;aws&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nb"&gt;load&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;balancer&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ssl&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ports&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"443"&lt;/span&gt;
    &lt;span class="n"&gt;external&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;dns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kubernetes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;hostname&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;connect&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;avilpage&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;com&lt;/span&gt;
&lt;span class="n"&gt;spec&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="n"&gt;type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;LoadBalancer&lt;/span&gt;
  &lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mirth&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;connect&lt;/span&gt;
  &lt;span class="n"&gt;ports&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;http&lt;/span&gt;
      &lt;span class="n"&gt;port&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt;
      &lt;span class="n"&gt;targetPort&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;8080&lt;/span&gt;
      &lt;span class="n"&gt;protocol&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;TCP&lt;/span&gt;
    &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;https&lt;/span&gt;
      &lt;span class="n"&gt;port&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;443&lt;/span&gt;
      &lt;span class="n"&gt;targetPort&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;8443&lt;/span&gt;
      &lt;span class="n"&gt;protocol&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;TCP&lt;/span&gt;
    &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;hl7&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;
      &lt;span class="n"&gt;port&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;9001&lt;/span&gt;
      &lt;span class="n"&gt;targetPort&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;9001&lt;/span&gt;
      &lt;span class="n"&gt;protocol&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;TCP&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will create a load balancer in AWS through which we can access mirth connect instance. If an ingress controller is present in the cluster, we can use it directly instead of using a seperate load balancer for this service.&lt;/p&gt;
&lt;p&gt;Once Mirth Connect is up &amp;amp; running, we might have to create HL7 channels running on various ports. In the above configuration files, we have exposed 9001 HL7 port for testing of channel. Once we configure Mirth Channels, we need to expose appropriate ports in deployment as well as service similiar to this.&lt;/p&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;Earlier, there were no official docker images for Mirth Connect and it was bit diffucult to dockerize Mirth Connect and deploy it. With the release of official Docker images, deploying Mirth Connect to k8s or any other container orchestration platform has become much easier.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:nc"&gt;
&lt;p&gt;&lt;a href="https://hub.docker.com/r/nextgenhealthcare/connect/"&gt;https://hub.docker.com/r/nextgenhealthcare/connect/&lt;/a&gt;Â &lt;a class="footnote-backref" href="http://avilpage.com/2020/07/deploy-mirth-to-kubernetes.html#fnref:nc" title="Jump back to footnote 1 in the text"&gt;â©&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>devops</category><category>health-care</category><category>kubernetes</category><category>mirth-connect</category><guid>http://avilpage.com/2020/07/deploy-mirth-to-kubernetes.html</guid><pubDate>Wed, 30 Sep 2020 17:55:46 GMT</pubDate></item><item><title>Mastering DICOM - Part #1</title><link>http://avilpage.com/2019/12/mastering-dicom-part-1.html</link><dc:creator>Chillar Anand</dc:creator><description>&lt;div&gt;&lt;h4&gt;Introduction&lt;/h4&gt;
&lt;p&gt;In hospitals, &lt;a href="https://en.wikipedia.org/wiki/Picture_archiving_and_communication_system"&gt;PACS&lt;/a&gt; simplifies the clinical workflow by reducing physical and time barriers. A typical radiology workflow looks like this.&lt;/p&gt;
&lt;p algin="center"&gt;
&lt;img src="http://avilpage.com/images/dicom-pacs-python1.png"&gt;
&lt;/p&gt;

&lt;p&gt;Credit: &lt;cite&gt;Wikipedia&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;A patient as per doctor's request will visit a radiology center to undergo CT/MRI/X-RAY. Data captured from modality(medical imaging equipments like CT/MRI machine) will be sent to QA for verfication and then sent to PACS for archiving.&lt;/p&gt;
&lt;p&gt;After this when patient visits doctor, doctor can see this study on his workstation(which has DICOM viewer) by entering patient details.&lt;/p&gt;
&lt;p&gt;In &lt;a href="http://avilpage.com/tags/dicom.html"&gt;this series of articles&lt;/a&gt;, we will how to achieve this seamless transfer of medical data digitally with DICOM.&lt;/p&gt;
&lt;h4&gt;DICOM standard&lt;/h4&gt;
&lt;p&gt;DICOM modalities create files in DICOM format. This file has dicom header which contains meta data and dicom data set which has modality info(equipment information, equipment configuration etc), patient information(name, sex etc) and the image data.&lt;/p&gt;
&lt;p&gt;Storing and retreiving DICOM files from PACS servers is generally achieved through DIMSE DICOM for desktop applications and DICOMWeb for web applications.&lt;/p&gt;
&lt;p&gt;All the machines which transfer/receive DICOM data must follow DICOM standard. With this all the DICOM machines which are in a network can store and retrieve DICOM files from PACS.&lt;/p&gt;
&lt;p&gt;When writing software to handle DICOM data, there are third party packages to handle most of the these things for us.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Python(&lt;a href="https://pypi.org/project/pydicom/"&gt;pydicom&lt;/a&gt;, &lt;a href="https://pypi.org/project/pynetdicom/"&gt;pynetdicom&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ruby (&lt;a href="https://rubygems.org/gems/dicom/"&gt;ruby-dicom&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;R (&lt;a href="https://CRAN.R-project.org/package=oro.dicom"&gt;oro.dicom&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;C/C++ (&lt;a href="https://dicom.offis.de/dcmtk"&gt;dcmtoolkit&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;In this article, we have learnt the clinical radiology workflow and how DICOM standard is useful in digitally transferring data between DICOM modalities.&lt;/p&gt;
&lt;p&gt;In the next article, we will dig into DICOM file formats and learn about the structure of DICOM data.&lt;/p&gt;&lt;/div&gt;</description><category>dicom</category><category>health-care</category><category>python</category><guid>http://avilpage.com/2019/12/mastering-dicom-part-1.html</guid><pubDate>Tue, 31 Dec 2019 13:29:09 GMT</pubDate></item></channel></rss>