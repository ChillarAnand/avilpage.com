<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Avil Page (Posts about scraping)</title><link>http://avilpage.com/</link><description></description><atom:link href="http://avilpage.com/tags/scraping.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Thu, 09 Dec 2021 02:53:49 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Convert Browser Requests To Python Requests For Scraping</title><link>http://avilpage.com/2018/03/convert-browser-requests-to-python-requests.html</link><dc:creator>Chillar Anand</dc:creator><description>&lt;div&gt;&lt;p&gt;Scraping content behind a login page is bit difficult as there are wide variety of authentication mechanisms and web server needs correct headers, session, cookies to authenticate the request.&lt;/p&gt;
&lt;p&gt;If we need a crawler which runs everyday to scrape content, then we have to implement authentication mechanism. If we need to quickly scrape content just for once, implementing authentication is an overhead.&lt;/p&gt;
&lt;p&gt;Instead, we can manually login to the website, capture an authenticated request and use it for scraping other pages by changing url/form parameters.&lt;/p&gt;
&lt;p&gt;From browser developer options, we can capture &lt;a href="https://curl.haxx.se/"&gt;curl&lt;/a&gt; equivalent command for any request from &lt;code&gt;Network&lt;/code&gt; tab with &lt;code&gt;copy as cURL&lt;/code&gt; option.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;img src="http://avilpage.com/images/requests-python-browser.png" height="300px" width="600"&gt;
&lt;/p&gt;

&lt;p&gt;Here is one such request.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;curl&lt;/span&gt; &lt;span class="s1"&gt;'http://avilpage.com/dummy'&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="s1"&gt;'Cookie: ASPSESSIONIDSABAAQDA=FKOHHAGAFODIIGNNNDFKNGLM'&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="s1"&gt;'Origin: http://avilpage.com'&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="s1"&gt;'Accept-Encoding: gzip, deflate'&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="s1"&gt;'Accept-Language: en-US,en;q=0.9,ms;q=0.8,te;q=0.7'&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="s1"&gt;'Upgrade-Insecure-Requests: 1'&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="s1"&gt;'User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="s1"&gt;'Content-Type: application/x-www-form-urlencoded'&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="s1"&gt;'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8'&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="s1"&gt;'Cache-Control: max-age=0'&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="s1"&gt;'Referer: http://avilpage.com/'&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="s1"&gt;'Connection: keep-alive'&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="s1"&gt;'DNT: 1'&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="s1"&gt;'page=2&amp;amp;category=python'&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;compressed&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once we get curl command, we can directly convert it to python requests using &lt;a href="https://pypi.python.org/pypi/uncurl/"&gt;uncurl&lt;/a&gt;.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ pip install uncurl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since the copied curl request is in clipboard, we can pipe it to uncurl.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;$&lt;/span&gt; &lt;span class="n"&gt;clipit&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;uncurl&lt;/span&gt;

&lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;post&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"http://avilpage.com/dummy"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'page=2&amp;amp;category=python'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"Accept"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"Accept-Encoding"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"gzip, deflate"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"Accept-Language"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"en-US,en;q=0.9,ms;q=0.8,te;q=0.7"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"Cache-Control"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"max-age=0"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"Content-Type"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"application/x-www-form-urlencoded"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"Origin"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"http://avilpage.com"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"Referer"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"http://avilpage.com/"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"Upgrade-Insecure-Requests"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"1"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"User-Agent"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36"&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="n"&gt;cookies&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"ASPSESSIONIDSABAAQDA"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"FKOHHAGAFODIIGNNNDFKNGLM"&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we have to use some other programming language, we can use &lt;a href="https://curl.trillworks.com/"&gt;curlconverter&lt;/a&gt; to convert curl command to Go or Node.js equivalent code.&lt;/p&gt;
&lt;p&gt;Now, we can use this code to get contents of current page and then continue scraping from the urls in it.&lt;/p&gt;&lt;/div&gt;</description><category>python</category><category>scraping</category><guid>http://avilpage.com/2018/03/convert-browser-requests-to-python-requests.html</guid><pubDate>Mon, 26 Mar 2018 15:51:21 GMT</pubDate></item><item><title>How To Use Python &amp; Beautiful Soup To Scrap Web Pages On Ubuntu</title><link>http://avilpage.com/2014/04/how-to-use-python-beautiful-soup-to.html</link><dc:creator>Chillar Anand</dc:creator><description>&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;&lt;span style="font-family: inherit;"&gt;In this tutorial, You will learn to install Beautiful Soup and parse any web page you like.&lt;/span&gt;&lt;br&gt;&lt;span style="font-family: inherit;"&gt;&lt;br&gt;&lt;/span&gt;&lt;h2 style="text-align: left;"&gt;&lt;span style="font-family: inherit;"&gt;Python &amp;amp; Beautiful Soup Installation On Ubuntu 12.04:&lt;/span&gt; &lt;/h2&gt;&lt;span style="font-family: inherit;"&gt;&lt;br&gt;&lt;/span&gt;&lt;span style="font-family: inherit;"&gt;1. Open your terminal ( Alt + Ctrl + T ). Install Python &amp;amp; Soup by using these commands.&lt;/span&gt;&lt;/div&gt;&lt;div dir="ltr" trbidi="on"&gt;&lt;div style="text-align: center;"&gt;&lt;pre style="box-sizing: border-box;"&gt;&lt;span style="font-family: inherit;"&gt;&lt;code class="bash hljs " style="background-color: #333333; border-bottom-left-radius: 6px; border-bottom-right-radius: 6px; border-top-left-radius: 6px; border-top-right-radius: 6px; box-shadow: rgba(0, 0, 0, 0.298039) 0px 1px 10px inset, rgba(255, 255, 255, 0.0980392) 0px 1px 0px, rgba(0, 0, 0, 0.498039) 0px -1px 0px; box-sizing: border-box; display: block; padding: 15px 20px; position: relative; text-align: left;"&gt;&lt;span style="color: #eeeeee;"&gt;&lt;div&gt;&lt;br&gt;sudo add-apt-repository ppa:fkrull/deadsnakes&lt;br&gt;&lt;br&gt;sudo apt-get update&lt;br&gt;&lt;br&gt;sudo apt-get install python2.7 &lt;br&gt;&lt;br&gt;sudo apt-get install python-bs4&lt;/div&gt;&lt;br&gt;&lt;/span&gt;&lt;/code&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;br&gt;&lt;h2 style="text-align: left;"&gt;&lt;span style="font-family: inherit;"&gt;Scraping A Web Page: &lt;/span&gt;&lt;/h2&gt;&lt;span style="font-family: inherit;"&gt;Lets start our programm by importing Beautiful soup. Since we are going to open a web page, we need urllib2 ( This is for Python 2, For Python 3 see urllib.request ). So import that library also.&lt;code&gt;&lt;span style="color: #eeeeee;"&gt; &lt;/span&gt;&lt;/code&gt;Select any url to parse. I have selected the home page of this blog and opened that page with urlopen().&lt;code&gt;&lt;span style="color: #eeeeee;"&gt; &lt;/span&gt;&lt;/code&gt;Pass the web page contents of 'page' variable to beautiful soup.&lt;code&gt;&lt;span style="color: #eeeeee;"&gt; &lt;/span&gt;&lt;/code&gt;Lets print all the links which are present in this page.&lt;/span&gt;&lt;br&gt;&lt;span style="font-family: inherit;"&gt;So, the final code is&lt;/span&gt;&lt;br&gt;&lt;div dir="ltr" trbidi="on"&gt;&lt;div style="text-align: center;"&gt;&lt;pre style="box-sizing: border-box;"&gt;&lt;span style="font-family: inherit;"&gt;&lt;code class="bash hljs " style="background-color: #333333; border-bottom-left-radius: 6px; border-bottom-right-radius: 6px; border-top-left-radius: 6px; border-top-right-radius: 6px; box-shadow: rgba(0, 0, 0, 0.298039) 0px 1px 10px inset, rgba(255, 255, 255, 0.0980392) 0px 1px 0px, rgba(0, 0, 0, 0.498039) 0px -1px 0px; box-sizing: border-box; display: block; padding: 15px 20px; position: relative; text-align: left;"&gt;&lt;span style="color: #eeeeee;"&gt;&lt;div&gt;&lt;br&gt;from bs4 import BeautifulSoup&lt;br&gt;import urllib2&lt;br&gt; &lt;br&gt;url = "https://www.goodreads.com/quotes/tag/love"&lt;br&gt;page = urllib2.urlopen(url)&lt;br&gt;soup = BeautifulSoup(page.read())&lt;br&gt; &lt;br&gt;for anchor in soup.find_all('a'):&lt;br&gt;    print(anchor.get('href', '/')) &lt;/div&gt;&lt;br&gt;&lt;/span&gt;&lt;/code&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;br&gt;&lt;span style="font-family: inherit;"&gt;Go ahead and run this file. I have name the file as scrap_web_page.py&lt;/span&gt;&lt;br&gt;&lt;div dir="ltr" trbidi="on"&gt;&lt;div style="text-align: center;"&gt;&lt;pre style="box-sizing: border-box;"&gt;&lt;span style="font-family: inherit;"&gt;&lt;code class="bash hljs " style="background-color: #333333; border-bottom-left-radius: 6px; border-bottom-right-radius: 6px; border-top-left-radius: 6px; border-top-right-radius: 6px; box-shadow: rgba(0, 0, 0, 0.298039) 0px 1px 10px inset, rgba(255, 255, 255, 0.0980392) 0px 1px 0px, rgba(0, 0, 0, 0.498039) 0px -1px 0px; box-sizing: border-box; display: block; padding: 15px 20px; position: relative; text-align: left;"&gt;&lt;span style="color: #eeeeee;"&gt;&lt;div&gt;&lt;br&gt;python scrap_web_page.py&lt;/div&gt;&lt;br&gt;&lt;/span&gt;&lt;/code&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;span style="font-family: inherit;"&gt;&lt;br&gt;&lt;/span&gt;&lt;span style="font-family: inherit;"&gt;It will produce all the links in the terminal itself. I have added a screenshot of the output I have received.&lt;/span&gt;&lt;br&gt;&lt;br&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://1.bp.blogspot.com/-QoTo6Nmu8IA/U16noONHUII/AAAAAAAAKas/uNAm2sooDO8/s1600/python-beautiful-soup-how-to.png" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"&gt;&lt;img border="0" src="http://1.bp.blogspot.com/-QoTo6Nmu8IA/U16noONHUII/AAAAAAAAKas/uNAm2sooDO8/s1600/python-beautiful-soup-how-to.png" height="277" width="640"&gt;&lt;/a&gt;&lt;/div&gt;&lt;div style="text-align: center;"&gt;&lt;br&gt;&lt;/div&gt;&lt;span style="font-family: inherit;"&gt;&lt;br&gt;&lt;/span&gt;&lt;span style="font-family: inherit;"&gt;anchor.get() is just one method to get all links, you can grab any element or class or name or anything from the page. For complete details, read the &lt;a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/" target="_blank"&gt;documentation of Beautiful Soup&lt;/a&gt;. &lt;/span&gt;&lt;/div&gt;</description><category>python</category><category>scraping</category><category>ubuntu</category><guid>http://avilpage.com/2014/04/how-to-use-python-beautiful-soup-to.html</guid><pubDate>Mon, 28 Apr 2014 13:43:00 GMT</pubDate></item></channel></rss>