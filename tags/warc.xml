<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Avil Page (Posts about warc)</title><link>http://avilpage.com/</link><description></description><atom:link href="http://avilpage.com/tags/warc.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sun, 18 Nov 2018 12:00:00 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Archive Million Pages With wget In Minutes</title><link>http://avilpage.com/2018/11/archive-millions-pages-wget-minutes.html</link><dc:creator>Chillar Anand</dc:creator><description>&lt;div&gt;&lt;h4&gt;Introduction&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://github.com/webrecorder/webrecorder"&gt;webrecorder&lt;/a&gt;, &lt;a href="https://github.com/internetarchive/heritrix3"&gt;heritrix&lt;/a&gt;, &lt;a href="https://nutch.apache.org/"&gt;nutch&lt;/a&gt;, &lt;a href="https://scrapy.org/"&gt;scrapy&lt;/a&gt;, &lt;a href="https://github.com/gocolly/colly"&gt;colly&lt;/a&gt;, &lt;a href="https://github.com/scrapinghub/frontera"&gt;frontera&lt;/a&gt; are popular tools for large scale web crawling and archiving.&lt;/p&gt;
&lt;p&gt;These tools require some learning curve and some of them don't have inbuilt support for warc(&lt;a href="https://en.wikipedia.org/wiki/Web_ARChive"&gt;Web ARChive&lt;/a&gt;) output format.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;wget&lt;/code&gt; comes bundled with most *nix systems and has inbuilt support for warc output. In this article we will see how to quickly archive web pages with wget.&lt;/p&gt;
&lt;h4&gt;Archiving with wget&lt;/h4&gt;
&lt;p&gt;In previous article we have extracted a &lt;a href="http://avilpage.com/2018/11/comparision-alexa-majestic-domcorp-top-million-sites.html"&gt;superset of top 1 million domains&lt;/a&gt;. We can use that list or urls to archive. Save this list to a file called &lt;code&gt;urls.txt&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This can be archived with the following command.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;urls.txt
wget -i &lt;span class="nv"&gt;$file&lt;/span&gt; --warc-file&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$file&lt;/span&gt; -t &lt;span class="m"&gt;3&lt;/span&gt; --timeout&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt; -q -o /dev/null -O /dev/null
&lt;/pre&gt;


&lt;p&gt;wget has the ability to continue partially downloaded files. But this option won't work with warc output. So, it is better to split this list into small chunks and process them. One added advantage of this approach is we can parallely download multiple chunks with wget.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;mkdir -p chunks
split -l &lt;span class="m"&gt;1000&lt;/span&gt; urls.txt chunks/ -d --additional-suffix&lt;span class="o"&gt;=&lt;/span&gt;.txt -a &lt;span class="m"&gt;3&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;This will split the file into several chunks each containing 1000 urls. wget doesn't have multithreading support. We can write a for loop to schedule a seperate process for each chunk.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;for file in `ls -r chunks/*.txt`
do
   wget -i $file --warc-file=$file -t 3 --timeout=4 -q -o /dev/null -O /dev/null &amp;amp;
done
&lt;/pre&gt;


&lt;p&gt;To archive 1000 urls, it takes ~15 minutes. In less than 20 minutes, it will download entire million pages.&lt;/p&gt;
&lt;p&gt;Also, each process takes ~8MB of memory. To run 1000 process, a system needs 8GB+ memory. Otherwise, number of parallel processes should be reduced which increases overall run time.&lt;/p&gt;
&lt;p&gt;Each archive chunk will be ~150MB and consume lot of storage. All downloaded acrhives can be zipped to reduce storage.&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;gzip *.warc
&lt;/pre&gt;


&lt;p&gt;Here is an idempotent shell script to download and archive files in batches.&lt;/p&gt;
&lt;table class="codehilitetable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#! /bin/sh&lt;/span&gt;

&lt;span class="nb"&gt;set&lt;/span&gt; -x

&lt;span class="nv"&gt;batch&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1000&lt;/span&gt;
&lt;span class="nv"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;expr &lt;span class="si"&gt;${#&lt;/span&gt;&lt;span class="nv"&gt;batch&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; - &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="nv"&gt;maxproc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;50&lt;/span&gt;
&lt;span class="nv"&gt;file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;urls.txt
&lt;span class="nv"&gt;dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;&lt;span class="s1"&gt;'/projects/chunks'&lt;/span&gt;&lt;span class="nv"&gt;$batch&lt;/span&gt;


mkdir -p &lt;span class="nv"&gt;$dir&lt;/span&gt;
split -l &lt;span class="nv"&gt;$batch&lt;/span&gt; &lt;span class="nv"&gt;$file&lt;/span&gt; &lt;span class="nv"&gt;$dir&lt;/span&gt;&lt;span class="s1"&gt;'/'&lt;/span&gt; -d --additional-suffix&lt;span class="o"&gt;=&lt;/span&gt;.txt -a &lt;span class="nv"&gt;$size&lt;/span&gt;
sleep &lt;span class="m"&gt;1&lt;/span&gt;

&lt;span class="nv"&gt;useragent&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'&lt;/span&gt;


&lt;span class="k"&gt;for&lt;/span&gt; file in &lt;span class="sb"&gt;`&lt;/span&gt;ls -r &lt;span class="nv"&gt;$dir&lt;/span&gt;/*.txt&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="k"&gt;do&lt;/span&gt;
    &lt;span class="nv"&gt;warcfile&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$file&lt;/span&gt;&lt;span class="s1"&gt;'.warc'&lt;/span&gt;
    &lt;span class="nv"&gt;warczip&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$warcfile&lt;/span&gt;&lt;span class="s1"&gt;'.gz'&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; -f &lt;span class="nv"&gt;$warczip&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; -f &lt;span class="nv"&gt;$warcfile&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
        &lt;span class="k"&gt;continue&lt;/span&gt;
    &lt;span class="k"&gt;fi&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;pgrep wget -c&lt;span class="k"&gt;)&lt;/span&gt; -lt &lt;span class="nv"&gt;$maxproc&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
        &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="nv"&gt;$file&lt;/span&gt;
        wget -H &lt;span class="s2"&gt;"user-agent: &lt;/span&gt;&lt;span class="nv"&gt;$useragent&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; -i &lt;span class="nv"&gt;$file&lt;/span&gt; --warc-file&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$file&lt;/span&gt; -t &lt;span class="m"&gt;3&lt;/span&gt; --timeout&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt; -q -o /dev/null -O /dev/null &lt;span class="p"&gt;&amp;amp;&lt;/span&gt;
        sleep &lt;span class="m"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;
        sleep &lt;span class="m"&gt;300&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; filename in &lt;span class="sb"&gt;`&lt;/span&gt;find &lt;span class="nv"&gt;$dir&lt;/span&gt; -name &lt;span class="s1"&gt;'*.warc'&lt;/span&gt; -mmin +5&lt;span class="sb"&gt;`&lt;/span&gt;
        &lt;span class="k"&gt;do&lt;/span&gt;
            gzip &lt;span class="nv"&gt;$filename&lt;/span&gt; -9
        &lt;span class="k"&gt;done&lt;/span&gt;
    &lt;span class="k"&gt;fi&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/pre&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;In this article, we have seen how to archive million pages with wget in few minutes.&lt;/p&gt;
&lt;p&gt;wget2 has multithreading support and &lt;a href="https://gitlab.com/gnuwget/wget2/issues/65"&gt;it might have warc output soon&lt;/a&gt;. With that, archiving with wget becomes much easier.&lt;/p&gt;&lt;/div&gt;</description><category>command-line</category><category>warc</category><guid>http://avilpage.com/2018/11/archive-millions-pages-wget-minutes.html</guid><pubDate>Sun, 18 Nov 2018 11:51:21 GMT</pubDate></item></channel></rss>