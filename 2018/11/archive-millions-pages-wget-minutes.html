<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="How to archive millions of pages without using any tools in few minutes.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Archive Million Pages With wget In Minutes | Avil Page</title>
<link href="../../assets/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/baguetteBox.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/theme.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../rss.xml">
<link rel="canonical" href="https://avilpage.com/2018/11/archive-millions-pages-wget-minutes.html">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Chillar Anand">
<link rel="prev" href="comparision-alexa-majestic-domcorp-top-million-sites.html" title="Alexa vs Domcop vs Majestic - Top Million Sites" type="text/html">
<link rel="next" href="../12/django-bottleneck-performance-scaling.html" title="Find High-impact Performance Bottlenecks in Django" type="text/html">
<meta property="og:site_name" content="Avil Page">
<meta property="og:title" content="Archive Million Pages With wget In Minutes">
<meta property="og:url" content="https://avilpage.com/2018/11/archive-millions-pages-wget-minutes.html">
<meta property="og:description" content="How to archive millions of pages without using any tools in few minutes.">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2018-11-18T17:21:21+05:30">
<meta property="article:tag" content="command-line">
<style>
    .full-article-footer {
    border-top: 1px solid #E0DFDB;
}


.article-footer {
    padding: 20px 10px 10px 10px;
    min-height: 115px;
    display:table;
}


.avatar-module {
    display: table-cell;
    vertical-align: middle;
}


.avatar-module img {
    border-radius: 50%!important;
    height: 120px;
    width: 120px;
    float: left;
    margin:0px 20px 0px 20px;
}


.article-footer p {
    line-height:1.5em;
    padding-left:15px;
}


@media (max-width:750px) {
    .article-footer {
        display:inherit;
        margin:0px;
    }

    .avatar-module {
        display:inherit;
        vertical-align: none;
    }

    .avatar-module img {
        margin-left: auto;
        margin-right: auto;
        display: block;
        float:none;
        margin-top:0px;
    }

    .article-footer p {
        text-align:center;
        padding-left:0px;
    }
}

    </style>
<style>

 h1 {
     font-weight: bold;
     font-size: 25px;
 }

</style>
</head>
<body>
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <!-- Menubar -->
    <nav class="navbar navbar-expand-md static-top mb-4
                navbar-dark
                bg-dark
                "><div class="container">
<!-- This keeps the margins nice -->
            <a class="navbar-brand" href="../../">

                    <span id="blog-title">Avil Page</span>
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="bs-navbar">
                <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="../../top-10.html" class="nav-link">Top 10</a>
                </li>
<li class="nav-item">
<a href="../../archive.html" class="nav-link">Tech</a>
                </li>
<li class="nav-item">
<a href="../../non-tech.html" class="nav-link">Non-tech</a>
                </li>
<li class="nav-item">
<a href="../../rss.xml" class="nav-link">RSS</a>

                    
                </li>
</ul>
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
        </div>
<!-- /.container -->
    </nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="row justify-content-md-center">
        <div class="body-content col col-lg-6">
            <!--Body content-->
            
    <article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="#" class="u-url">Archive Million Pages With wget In Minutes</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card">
                <span class="byline-name fn p-name" itemprop="author">
                    Chillar Anand
                </span>
            </p>

            <p class="dateline">
                    8 min read
            </p>

            <p class="dateline">
                <a href="#" rel="bookmark">
                    <time class="published dt-published" datetime="2018-11-18T17:21:21+05:30" itemprop="datePublished" title="2018-11-18">2018-11-18</time></a>
            </p>
            
        </div>

        
    </header><div class="e-content entry-content" itemprop="articleBody text">
            <h4>Introduction</h4>
<p><a href="https://github.com/webrecorder/webrecorder">webrecorder</a>, <a href="https://github.com/internetarchive/heritrix3">heritrix</a>, <a href="https://nutch.apache.org/">nutch</a>, <a href="https://scrapy.org/">scrapy</a>, <a href="https://github.com/gocolly/colly">colly</a>, <a href="https://github.com/scrapinghub/frontera">frontera</a> are popular tools for large scale web crawling and archiving.</p>
<p>These tools require some learning curve and some of them don't have inbuilt support for warc(<a href="https://en.wikipedia.org/wiki/Web_ARChive">Web ARChive</a>) output format.</p>
<p><code>wget</code> comes bundled with most *nix systems and has inbuilt support for warc output. In this article we will see how to quickly archive web pages with wget.</p>
<h4>Archiving with wget</h4>
<p>In previous article we have extracted a <a href="comparision-alexa-majestic-domcorp-top-million-sites.html">superset of top 1 million domains</a>. We can use that list or urls to archive. Save this list to a file called <code>urls.txt</code>.</p>
<p>This can be archived with the following command.</p>
<div class="code"><pre class="code literal-block"><span class="nv">file</span><span class="o">=</span>urls.txt
wget<span class="w"> </span>-i<span class="w"> </span><span class="nv">$file</span><span class="w"> </span>--warc-file<span class="o">=</span><span class="nv">$file</span><span class="w"> </span>-t<span class="w"> </span><span class="m">3</span><span class="w"> </span>--timeout<span class="o">=</span><span class="m">4</span><span class="w"> </span>-q<span class="w"> </span>-o<span class="w"> </span>/dev/null<span class="w"> </span>-O<span class="w"> </span>/dev/null
</pre></div>

<p>wget has the ability to continue partially downloaded files. But this option won't work with warc output. So, it is better to split this list into small chunks and process them. One added advantage of this approach is we can parallely download multiple chunks with wget.</p>
<div class="code"><pre class="code literal-block">mkdir<span class="w"> </span>-p<span class="w"> </span>chunks
split<span class="w"> </span>-l<span class="w"> </span><span class="m">1000</span><span class="w"> </span>urls.txt<span class="w"> </span>chunks/<span class="w"> </span>-d<span class="w"> </span>--additional-suffix<span class="o">=</span>.txt<span class="w"> </span>-a<span class="w"> </span><span class="m">3</span>
</pre></div>

<p>This will split the file into several chunks each containing 1000 urls. wget doesn't have multithreading support. We can write a for loop to schedule a seperate process for each chunk.</p>
<div class="code"><pre class="code literal-block"><span class="k">for</span><span class="w"> </span><span class="nv">file</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span>`<span class="nv">ls</span><span class="w"> </span><span class="o">-</span><span class="nv">r</span><span class="w"> </span><span class="nv">chunks</span><span class="cm">/*.txt`</span>
<span class="cm">do</span>
<span class="cm">   wget -i $file --warc-file=$file -t 3 --timeout=4 -q -o /dev/null -O /dev/null &amp;</span>
<span class="cm">done</span>
</pre></div>

<p>To archive 1000 urls, it takes ~15 minutes. In less than 20 minutes, it will download entire million pages.</p>
<p>Also, each process takes ~8MB of memory. To run 1000 process, a system needs 8GB+ memory. Otherwise, number of parallel processes should be reduced which increases overall run time.</p>
<p>Each archive chunk will be ~150MB and consume lot of storage. All downloaded acrhives can be zipped to reduce storage.</p>
<div class="code"><pre class="code literal-block">gzip<span class="w"> </span>*.warc
</pre></div>

<p>Here is an idempotent shell script to download and archive files in batches.</p>
<div class="code"><pre class="code literal-block"><span class="ch">#! /bin/sh</span>

<span class="nb">set</span><span class="w"> </span>-x

<span class="nv">batch</span><span class="o">=</span><span class="m">1000</span>
<span class="nv">size</span><span class="o">=</span><span class="sb">`</span>expr<span class="w"> </span><span class="si">${#</span><span class="nv">batch</span><span class="si">}</span><span class="w"> </span>-<span class="w"> </span><span class="m">1</span><span class="sb">`</span>
<span class="nv">maxproc</span><span class="o">=</span><span class="m">50</span>
<span class="nv">file</span><span class="o">=</span>urls.txt
<span class="nv">dir</span><span class="o">=</span><span class="nv">$HOME</span><span class="s1">'/projects/chunks'</span><span class="nv">$batch</span>


mkdir<span class="w"> </span>-p<span class="w"> </span><span class="nv">$dir</span>
split<span class="w"> </span>-l<span class="w"> </span><span class="nv">$batch</span><span class="w"> </span><span class="nv">$file</span><span class="w"> </span><span class="nv">$dir</span><span class="s1">'/'</span><span class="w"> </span>-d<span class="w"> </span>--additional-suffix<span class="o">=</span>.txt<span class="w"> </span>-a<span class="w"> </span><span class="nv">$size</span>
sleep<span class="w"> </span><span class="m">1</span>

<span class="nv">useragent</span><span class="o">=</span><span class="s1">'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'</span>


<span class="k">for</span><span class="w"> </span>file<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="sb">`</span>ls<span class="w"> </span>-r<span class="w"> </span><span class="nv">$dir</span>/*.txt<span class="sb">`</span>
<span class="k">do</span>
<span class="w">    </span><span class="nv">warcfile</span><span class="o">=</span><span class="nv">$file</span><span class="s1">'.warc'</span>
<span class="w">    </span><span class="nv">warczip</span><span class="o">=</span><span class="nv">$warcfile</span><span class="s1">'.gz'</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>-f<span class="w"> </span><span class="nv">$warczip</span><span class="w"> </span><span class="o">]</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>-f<span class="w"> </span><span class="nv">$warcfile</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">        </span><span class="k">continue</span>
<span class="w">    </span><span class="k">fi</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="k">$(</span>pgrep<span class="w"> </span>wget<span class="w"> </span>-c<span class="k">)</span><span class="w"> </span>-lt<span class="w"> </span><span class="nv">$maxproc</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">        </span><span class="nb">echo</span><span class="w"> </span><span class="nv">$file</span>
<span class="w">        </span>wget<span class="w"> </span>-H<span class="w"> </span><span class="s2">"user-agent: </span><span class="nv">$useragent</span><span class="s2">"</span><span class="w"> </span>-i<span class="w"> </span><span class="nv">$file</span><span class="w"> </span>--warc-file<span class="o">=</span><span class="nv">$file</span><span class="w"> </span>-t<span class="w"> </span><span class="m">3</span><span class="w"> </span>--timeout<span class="o">=</span><span class="m">4</span><span class="w"> </span>-q<span class="w"> </span>-o<span class="w"> </span>/dev/null<span class="w"> </span>-O<span class="w"> </span>/dev/null<span class="w"> </span><span class="p">&amp;</span>
<span class="w">        </span>sleep<span class="w"> </span><span class="m">2</span>
<span class="w">    </span><span class="k">else</span>
<span class="w">        </span>sleep<span class="w"> </span><span class="m">300</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span>filename<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="sb">`</span>find<span class="w"> </span><span class="nv">$dir</span><span class="w"> </span>-name<span class="w"> </span><span class="s1">'*.warc'</span><span class="w"> </span>-mmin<span class="w"> </span>+5<span class="sb">`</span>
<span class="w">        </span><span class="k">do</span>
<span class="w">            </span>gzip<span class="w"> </span><span class="nv">$filename</span><span class="w"> </span>-9
<span class="w">        </span><span class="k">done</span>
<span class="w">    </span><span class="k">fi</span>
<span class="k">done</span>
</pre></div>

<h4>Conclusion</h4>
<p>In this article, we have seen how to archive million pages with wget in few minutes.</p>
<p>wget2 has multithreading support and <a href="https://gitlab.com/gnuwget/wget2/issues/65">it might have warc output soon</a>. With that, archiving with wget becomes much easier.</p>
        </div>
        <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../tags/command-line.html" rel="tag">command-line</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="comparision-alexa-majestic-domcorp-top-million-sites.html" rel="prev" title="Alexa vs Domcop vs Majestic - Top Million Sites">Previous post</a>
            </li>
            <li class="next">
                <a href="../12/django-bottleneck-performance-scaling.html" rel="next" title="Find High-impact Performance Bottlenecks in Django">Next post</a>
            </li>
        </ul></nav></aside><div class="full-article-footer">
            <div class="article-footer">

                <div class="avatar-module">
                    <img class="avatar" height="100px" src="../../images/chillaranand.jpg">
</div>

                <p class="avatar-module">
                    <b>Chillar Anand</b>
                    <br>
                    A blog about python, careers &amp; life.
                    <br>
                    To contact me, <a href="https://forms.gle/Hre4z4aLqJA5zYWe6">send a message here</a>.
                </p>

            </div>
        </div>

        
    </article><!--End of body content--><footer id="footer"><div class="container align-items-center justify-content-center d-flex">

<footer class="footer"><a href="https://github.com/ChillarAnand">
<img src="../../images/icons8-github.svg" alt="github-chillar-anand" height="34"></a>

  

<a href="https://stackoverflow.com/users/2698552/chillar-anand">
<img src="../../images/icons8-so.svg" alt="github-chillar-anand" height="30"></a>

  

<a href="https://youtube.com/@avilpage">
<img src="../../images/icons8-youtube.svg" alt="github-chillar-anand" height="34"></a>

  

<a href="https://linkedin.com/in/chillaranand">
<img src="../../images/icons8-linkedin.svg" alt="github-chillar-anand" height="34"></a>

  

<a href="https://twitter.com/chillaranand">
<img src="../../images/icons8-twitter.svg" alt="github-chillar-anand" height="34"></a>

</footer>
</div>

<br><br></footer>
</div>
    </div>
    </div>

                <script src="../../assets/js/jquery.min.js"></script><script src="../../assets/js/popper.min.js"></script><script src="../../assets/js/bootstrap.min.js"></script><script src="../../assets/js/baguetteBox.min.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script>
</body>
</html>
