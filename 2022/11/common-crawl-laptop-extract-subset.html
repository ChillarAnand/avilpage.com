<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="How to process entire common crawl data set from your local machine.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Common Crawl On Laptop - Extracting Subset Of Data | Avil Page</title>
<link href="../../assets/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/baguetteBox.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/theme.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../rss.xml">
<link rel="canonical" href="https://avilpage.com/2022/11/common-crawl-laptop-extract-subset.html">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Chillar Anand">
<link rel="prev" href="build-distribute-a-python-c-extension-module.html" title="Build &amp; Distribute a Python C Extension Module" type="text/html">
<link rel="next" href="../12/common-crawl-laptop-web-directory.html" title="Common Crawl on Laptop - Building Web Directory" type="text/html">
<meta property="og:site_name" content="Avil Page">
<meta property="og:title" content="Common Crawl On Laptop - Extracting Subset Of Data">
<meta property="og:url" content="https://avilpage.com/2022/11/common-crawl-laptop-extract-subset.html">
<meta property="og:description" content="How to process entire common crawl data set from your local machine.">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2022-11-17T06:41:39+05:30">
<meta property="article:tag" content="command-line">
<meta property="article:tag" content="common-crawl">
<meta property="article:tag" content="data-analysis">
<style>
    .full-article-footer {
    border-top: 1px solid #E0DFDB;
}


.article-footer {
    padding: 20px 10px 10px 10px;
    min-height: 115px;
    display:table;
}


.avatar-module {
    display: table-cell;
    vertical-align: middle;
}


.avatar-module img {
    border-radius: 50%!important;
    height: 120px;
    width: 120px;
    float: left;
    margin:0px 20px 0px 20px;
}


.article-footer p {
    line-height:1.5em;
    padding-left:15px;
}


@media (max-width:750px) {
    .article-footer {
        display:inherit;
        margin:0px;
    }

    .avatar-module {
        display:inherit;
        vertical-align: none;
    }

    .avatar-module img {
        margin-left: auto;
        margin-right: auto;
        display: block;
        float:none;
        margin-top:0px;
    }

    .article-footer p {
        text-align:center;
        padding-left:0px;
    }
}

    </style>
<style>

 h1 {
     font-weight: bold;
     font-size: 25px;
 }

</style>
</head>
<body>
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <!-- Menubar -->
    <nav class="navbar navbar-expand-md static-top mb-4
                navbar-dark
                bg-dark
                "><div class="container">
<!-- This keeps the margins nice -->
            <a class="navbar-brand" href="../../">

                    <span id="blog-title">Avil Page</span>
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="bs-navbar">
                <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="../../archive.html" class="nav-link">Archive</a>
                </li>
<li class="nav-item">
<a href="../../top-10.html" class="nav-link">Top 10</a>
                </li>
<li class="nav-item">
<a href="../../rss.xml" class="nav-link">RSS</a>

                    
                </li>
</ul>
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
        </div>
<!-- /.container -->
    </nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="row justify-content-md-center">
        <div class="body-content col col-lg-6">
            <!--Body content-->
            
    <article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="#" class="u-url">Common Crawl On Laptop - Extracting Subset Of Data</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card">
                <span class="byline-name fn p-name" itemprop="author">
                    Chillar Anand
                </span>
            </p>

            <p class="dateline">
                    13 min read
            </p>

            <p class="dateline">
                <a href="#" rel="bookmark">
                    <time class="published dt-published" datetime="2022-11-17T06:41:39+05:30" itemprop="datePublished" title="2022-11-17">2022-11-17</time></a>
            </p>
            
        </div>

        
    </header><div class="e-content entry-content" itemprop="articleBody text">
            <p>This series of posts discuss processing of common crawl dataset on laptop.</p>
<ol>
<li>
<a href="#">Extracting Subset of Common Crawl</a> (this post)</li>
<li><a href="../12/common-crawl-laptop-web-directory.html">Building web directory</a></li>
</ol>
<h4>Introduction</h4>
<p>Common Crawl(CC)<sup id="fnref:common-crawl"><a class="footnote-ref" href="common-crawl-laptop-extract-subset.html#fn:common-crawl">1</a></sup> is an open repository of web containing peta bytes of data since 2008. As the dataset is huge, most of the tutorials use AWS EMR/Athena to process the data.</p>
<p>In this post, let's learn how to extract a subset of data(entire telugu language web pages) and process it on our local machine.</p>
<h4>Exploring Common Crawl</h4>
<p>CC provides monthly data dumps in WARC format. Each crawl consists of about ~3 billion web pages with a compressed size of ~100 TB.</p>
<p>In addition to WARC files, CC provides index files as well as columnar index<sup id="fnref:columnar-index-wiki"><a class="footnote-ref" href="common-crawl-laptop-extract-subset.html#fn:columnar-index-wiki">2</a></sup> files so that users can easily search, filter and download the data.</p>
<h4>Common Crawl Index</h4>
<p>Each crawl index is spread over 300 files consisting of ~250 GB of data. For this post, let use the latest crawl which is <code>CC-MAIN-2022-40</code>.</p>
<p>The index files can be accessed from AWS S3 or https. We can use aws cli to list all the files along with the sizes.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>aws<span class="w"> </span>s3<span class="w"> </span>ls<span class="w"> </span>--recursive<span class="w"> </span>--human-readable<span class="w"> </span>--summarize<span class="w"> </span>s3://commoncrawl/cc-index/collections/CC-MAIN-2022-40
<span class="m">2022</span>-10-08<span class="w"> </span><span class="m">16</span>:07:59<span class="w">  </span><span class="m">621</span>.9<span class="w"> </span>MiB<span class="w"> </span>cc-index/collections/CC-MAIN-2022-40/indexes/cdx-00000.gz
<span class="m">2022</span>-10-08<span class="w"> </span><span class="m">16</span>:08:26<span class="w">  </span><span class="m">721</span>.6<span class="w"> </span>MiB<span class="w"> </span>cc-index/collections/CC-MAIN-2022-40/indexes/cdx-00001.gz
...
<span class="m">2022</span>-10-08<span class="w"> </span><span class="m">16</span>:42:39<span class="w">  </span><span class="m">146</span>.6<span class="w"> </span>MiB<span class="w"> </span>cc-index/collections/CC-MAIN-2022-40/indexes/cluster.idx
<span class="m">2022</span>-10-08<span class="w"> </span><span class="m">16</span>:42:33<span class="w">   </span><span class="m">30</span><span class="w"> </span>Bytes<span class="w"> </span>cc-index/collections/CC-MAIN-2022-40/metadata.yaml

Total<span class="w"> </span>Objects:<span class="w"> </span><span class="m">302</span>
<span class="w">   </span>Total<span class="w"> </span>Size:<span class="w"> </span><span class="m">236</span>.1<span class="w"> </span>GiB
</pre></div>

<p>Let's download an index file to our local machine and see how the data is arranged. We can use <code>aws</code> cli to download the data from s3 bucket or use wget to download it from https endpoint.</p>
<div class="code"><pre class="code literal-block"><span class="c1"># from s3</span>
$<span class="w"> </span>aws<span class="w"> </span>s3<span class="w"> </span>cp<span class="w"> </span>s3://commoncrawl/cc-index/collections/CC-MAIN-2022-40/indexes/cdx-00000.gz<span class="w"> </span>.

<span class="c1"># from https</span>
$<span class="w"> </span>wget<span class="w"> </span>https://data.commoncrawl.org/cc-index/collections/CC-MAIN-2022-40/indexes/cdx-00000.gz
</pre></div>

<p>Let's print top five lines of the file.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>zcat<span class="w"> </span>&lt;<span class="w"> </span>cdx-00000.gz<span class="w"> </span><span class="p">|</span><span class="w"> </span>head<span class="w"> </span>-n<span class="w"> </span><span class="m">5</span>
<span class="m">0</span>,1,184,137<span class="o">)</span>/1klikbet<span class="w"> </span><span class="m">20221005193707</span><span class="w"> </span><span class="o">{</span><span class="s2">"url"</span>:<span class="w"> </span><span class="s2">"http://137.184.1.0/1klikbet/"</span>,<span class="w"> </span><span class="s2">"mime"</span>:<span class="w"> </span><span class="s2">"text/html"</span>,<span class="w"> </span><span class="s2">"mime-detected"</span>:<span class="w"> </span><span class="s2">"text/html"</span>,<span class="w"> </span><span class="s2">"status"</span>:<span class="w"> </span><span class="s2">"200"</span>,<span class="w"> </span><span class="s2">"digest"</span>:<span class="w"> </span><span class="s2">"XTKGORHKLZCHDBBOMYCYYIZVRPMXNRII"</span>,<span class="w"> </span><span class="s2">"length"</span>:<span class="w"> </span><span class="s2">"7065"</span>,<span class="w"> </span><span class="s2">"offset"</span>:<span class="w"> </span><span class="s2">"83437"</span>,<span class="w"> </span><span class="s2">"filename"</span>:<span class="w"> </span><span class="s2">"crawl-data/CC-MAIN-2022-40/segments/1664030337663.75/warc/CC-MAIN-20221005172112-20221005202112-00011.warc.gz"</span>,<span class="w"> </span><span class="s2">"charset"</span>:<span class="w"> </span><span class="s2">"UTF-8"</span>,<span class="w"> </span><span class="s2">"languages"</span>:<span class="w"> </span><span class="s2">"ind"</span><span class="o">}</span>
<span class="m">0</span>,1,184,137<span class="o">)</span>/7meter<span class="w"> </span><span class="m">20221005192131</span><span class="w"> </span><span class="o">{</span><span class="s2">"url"</span>:<span class="w"> </span><span class="s2">"http://137.184.1.0/7meter/"</span>,<span class="w"> </span><span class="s2">"mime"</span>:<span class="w"> </span><span class="s2">"text/html"</span>,<span class="w"> </span><span class="s2">"mime-detected"</span>:<span class="w"> </span><span class="s2">"text/html"</span>,<span class="w"> </span><span class="s2">"status"</span>:<span class="w"> </span><span class="s2">"200"</span>,<span class="w"> </span><span class="s2">"digest"</span>:<span class="w"> </span><span class="s2">"KUJAMRT6MXYR3RTWRJTIWJ5T2ZUB3EBH"</span>,<span class="w"> </span><span class="s2">"length"</span>:<span class="w"> </span><span class="s2">"7456"</span>,<span class="w"> </span><span class="s2">"offset"</span>:<span class="w"> </span><span class="s2">"142680"</span>,<span class="w"> </span><span class="s2">"filename"</span>:<span class="w"> </span><span class="s2">"crawl-data/CC-MAIN-2022-40/segments/1664030337663.75/warc/CC-MAIN-20221005172112-20221005202112-00182.warc.gz"</span>,<span class="w"> </span><span class="s2">"charset"</span>:<span class="w"> </span><span class="s2">"UTF-8"</span>,<span class="w"> </span><span class="s2">"languages"</span>:<span class="w"> </span><span class="s2">"ind"</span><span class="o">}</span>
...
</pre></div>

<p>The last column of each line contains the language information. We can use these index files, and we can  extract all the lines containing <code>tel</code> language code.</p>
<h4>Columnar Index</h4>
<p>We can also use columnar index to filter out telugu language web pages. Let's download a single file from the index.</p>
<div class="code"><pre class="code literal-block"><span class="c1"># from s3</span>
$<span class="w"> </span>aws<span class="w"> </span>s3<span class="w"> </span>cp<span class="w"> </span>s3://commoncrawl/cc-index/table/cc-main/warc/crawl<span class="o">=</span>CC-MAIN-2022-40/subset<span class="o">=</span>warc/part-00000-26160df0-1827-4787-a515-95ecaa2c9688.c000.gz.parquet<span class="w"> </span>.

<span class="c1"># from https</span>
$<span class="w"> </span>wget<span class="w"> </span>https://data.commoncrawl.org/cc-index/table/cc-main/warc/crawl<span class="o">=</span>CC-MAIN-2022-40/subset<span class="o">=</span>warc/part-00000-26160df0-1827-4787-a515-95ecaa2c9688.c000.gz.parquet
</pre></div>

<p>We can use Python pandas to read the parquet file and filter out telugu language web pages. Columnar index has <code>content_languages</code> column which can be used to filter out telugu pages as shown below.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">"""</span>
<span class="s2">import pandas as pd</span>
<span class="s2">filename = 'part-00000-26160df0-1827-4787-a515-95ecaa2c9688.c000.gz.parquet'</span>
<span class="s2">df = pd.read_parquet(filename)</span>
<span class="s2">df = df[df['content_languages'].str.startswith('tel', na=False)]</span>
<span class="s2">df.to_csv('telugu.csv')</span>
<span class="s2">"""</span>
</pre></div>

<p>I have used Macbook M1 with local ISP(Internet Service Provider) to download and extract the index. It took around 7 minutes to download a single file and 2 minutes to extract the data. To process 300 index files, it takes ~2 days.</p>
<p>Let's see how we can speed it up.</p>
<h4>Improving Performance</h4>
<h5>Faster Downloads</h5>
<p>My Wi-Fi speed is ~4MBps when downloading the index file. To download faster, I have created t2.micro(free-tier) EC2 instance on AWS. In this machine, download speed is ~10MBps. We can use other instances, but I am trying to use only free resources. In this machine, single file download is taking ~3 minutes.</p>
<p>CC dataset is hosted in us-east-1 region. So, I have created a new t2.micro instance in us-east-1 region. This instance is taking &lt;20 seconds to download a single file. We can download entire index in less than 2 hours.</p>
<h5>Faster Performance</h5>
<p>To extract data from index files, we have used Python pandas without specifying the engine. By default, it uses <code>pyarrow</code> which is a bit slow. To improve speed we can use <code>fastparquet</code> as engine which is ~5x faster than <code>pyarrow</code>.</p>
<div class="code"><pre class="code literal-block"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">filename</span> <span class="o">=</span> <span class="s1">'part-00000-26160df0-1827-4787-a515-95ecaa2c9688.c000.gz.parquet'</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">engine</span><span class="o">=</span><span class="s1">'fastparquet'</span><span class="p">)</span>
</pre></div>

<p>To get better performance, we can use duckdb. Duckdb is an in-process SQL OLAP DBMS and it can execute SQL queries directly on parquet files with <code>parquet</code> extension.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>brew<span class="w"> </span>install<span class="w"> </span>duckdb

$<span class="w"> </span>duckdb<span class="w"> </span>-c<span class="w"> </span><span class="s1">'INSTALL parquet;'</span>
</pre></div>

<p>We can write a simple SQL query to filter out the required rows.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>duckdb<span class="w"> </span>-c<span class="w"> </span><span class="s2">"""</span>
<span class="s2">LOAD parquet;</span>
<span class="s2">COPY (select * from PARQUET_SCAN('part-00000-26160df0-1827-4787-a515-95ecaa2c9688.c000.gz.parquet') where content_languages ilike '%tel%') TO 'telugu.csv' (DELIMITER ',', HEADER TRUE);</span>
<span class="s2">"""</span>
</pre></div>

<p>Duckdb can execute SQL queries on remote files as well with <code>httpfs</code> extension.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>duckdb<span class="w"> </span>-c<span class="w"> </span><span class="s1">'INSTALL httpfs;'</span>

$<span class="w"> </span>duckdb<span class="w"> </span>-c<span class="w"> </span><span class="s2">"""</span>
<span class="s2">    LOAD httpfs;</span>
<span class="s2">    LOAD parquet;</span>

<span class="s2">    COPY (select * from PARQUET_SCAN('s3://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2022-40/subset=warc/part-00001-26160df0-1827-4787-a515-95ecaa2c9688.c000.gz.parquet') where content_languages ilike '%tel%') TO 'telugu.csv' (DELIMITER ',', HEADER TRUE);"""</span>
<span class="s2">"""</span>
</pre></div>

<p>Duckdb can also read series of parquet files and treat them as a single table. We can use this feature to process all the index files in a single command.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>duckdb<span class="w"> </span>-c<span class="w"> </span><span class="s2">"""</span>
<span class="s2">    LOAD httpfs;</span>
<span class="s2">    LOAD parquet;</span>

<span class="s2">    SET s3_region='us-east-1';</span>
<span class="s2">    SET s3_access_key_id='s3_secret_access_key';</span>
<span class="s2">    SET s3_secret_access_key='s3_secret_access_key';</span>

<span class="s2">    COPY (select * from PARQUET_SCAN('s3://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2022-40/subset=warc/*.parquet') where content_languages ilike '%tel%') TO 'telugu.csv' (DELIMITER ',', HEADER TRUE);</span>
<span class="s2">"""</span>
</pre></div>

<p>Depending on the file size, duckdb takes 10-15 seconds to process a single file. Since we don't need all the columns for further data processing, we can limit columns to required 5 columns.</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>duckdb<span class="w"> </span>-c<span class="w"> </span><span class="s2">"""</span>
<span class="s2">    COPY (select url, content_languages, warc_filename, warc_record_offset, warc_record_length from PARQUET_SCAN('s3://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2022-40/subset=warc/*.parquet') where content_languages ilike '%tel%') TO 'telugu.csv' (DELIMITER ',', HEADER TRUE);</span>
<span class="s2">"""</span>
</pre></div>

<p>By limiting columns<sup id="fnref:cc-gg"><a class="footnote-ref" href="common-crawl-laptop-extract-subset.html#fn:cc-gg">3</a></sup> there is another 65% improvement in performance. Now duckdb can process a file in 3 to 8 seconds depending on the size of the file. We can process entire index in ~20 minutes.</p>
<h4>Conclusion</h4>
<p>With a single command, we can extract a subset of index from CC in ~2 hours. So far we have processed all files in a single process. We can also parallelize the process using <code>parallel</code> to get faster results.</p>
<p>In the upcoming posts, let's see how we can fetch the data from WARC files using this index and do further data processing.</p>
<div class="footnote">
<hr>
<ol>
<li id="fn:common-crawl">
<p><a href="https://commoncrawl.org">https://commoncrawl.org</a> <a class="footnote-backref" href="common-crawl-laptop-extract-subset.html#fnref:common-crawl" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
<li id="fn:columnar-index-wiki">
<p><a href="https://en.wikipedia.org/wiki/Column-oriented_DBMS">https://en.wikipedia.org/wiki/Column-oriented_DBMS</a> <a class="footnote-backref" href="common-crawl-laptop-extract-subset.html#fnref:columnar-index-wiki" title="Jump back to footnote 2 in the text">↩</a></p>
</li>
<li id="fn:cc-gg">
<p><a href="https://groups.google.com/g/common-crawl/c/WYwkW97RM4s">https://groups.google.com/g/common-crawl/c/WYwkW97RM4s</a> <a class="footnote-backref" href="common-crawl-laptop-extract-subset.html#fnref:cc-gg" title="Jump back to footnote 3 in the text">↩</a></p>
</li>
</ol>
</div>
        </div>
        <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../tags/command-line.html" rel="tag">command-line</a></li>
            <li><a class="tag p-category" href="../../tags/common-crawl.html" rel="tag">common-crawl</a></li>
            <li><a class="tag p-category" href="../../tags/data-analysis.html" rel="tag">data-analysis</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="build-distribute-a-python-c-extension-module.html" rel="prev" title="Build &amp; Distribute a Python C Extension Module">Previous post</a>
            </li>
            <li class="next">
                <a href="../12/common-crawl-laptop-web-directory.html" rel="next" title="Common Crawl on Laptop - Building Web Directory">Next post</a>
            </li>
        </ul></nav></aside><div class="full-article-footer">
            <div class="article-footer">

                <div class="avatar-module">
                    <img class="avatar" height="100px" src="../../images/chillaranand.jpg">
</div>

                <p class="avatar-module">
                    <b>Chillar Anand</b>
                    <br>
                    A blog about python, careers &amp; life.
                    <br>
                    To contact me, <a href="https://forms.gle/Hre4z4aLqJA5zYWe6">send a message here</a>.
                </p>

            </div>
        </div>

        
    </article><!--End of body content--><footer id="footer"><div class="container align-items-center justify-content-center d-flex">

<footer class="footer"><a href="https://github.com/ChillarAnand">
<img src="../../images/icons8-github.svg" alt="github-chillar-anand" height="34"></a>

  

<a href="https://stackoverflow.com/users/2698552/chillar-anand">
<img src="../../images/icons8-so.svg" alt="github-chillar-anand" height="30"></a>

  

<a href="https://youtube.com/@avilpage">
<img src="../../images/icons8-youtube.svg" alt="github-chillar-anand" height="34"></a>

  

<a href="https://linkedin.com/in/chillaranand">
<img src="../../images/icons8-linkedin.svg" alt="github-chillar-anand" height="34"></a>

  

<a href="https://twitter.com/chillaranand">
<img src="../../images/icons8-twitter.svg" alt="github-chillar-anand" height="34"></a>

</footer>
</div>

<br><br></footer>
</div>
    </div>
    </div>

                <script src="../../assets/js/jquery.min.js"></script><script src="../../assets/js/popper.min.js"></script><script src="../../assets/js/bootstrap.min.js"></script><script src="../../assets/js/baguetteBox.min.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script>
</body>
</html>
